#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
ì›ë³¸ ë²„ì „ê³¼ M4 Pro ìµœì í™” ë²„ì „ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""

import time
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset
from mps_utils import warm_up_mps, get_optimal_device


def benchmark_model_loading():
    """ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë¹„êµ"""
    print("ğŸ” ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë¹„êµ")
    print("=" * 50)

    # MPS warm-up
    warm_up_mps()
    device = get_optimal_device()

    # ì›ë³¸ ë²„ì „ (ê¸°ë³¸ ì„¤ì •)
    print("\nğŸ“Š ì›ë³¸ ë²„ì „ (ê¸°ë³¸ ì„¤ì •):")
    start_time = time.time()

    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased")
    model = model.to(device)

    end_time = time.time()
    print(f"   ë¡œë”© ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   ëª¨ë¸ í¬ê¸°: {sum(p.numel() * p.element_size()
          for p in model.parameters()) / 1024**2:.2f}MB")

    return model, tokenizer


def benchmark_inference_speed(model, tokenizer, device):
    """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸš€ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_texts = [
        "This movie is absolutely fantastic!",
        "I would put this at the top of the list of films in the category of unwatchable trash.",
        "I can watch this all day.",
        "Terrible acting and boring plot.",
        "The best movie I have ever seen in my entire life.",
        "Waste of time and money.",
        "Outstanding performance by all actors.",
        "Boring and predictable storyline.",
        "A masterpiece of cinema.",
        "Complete garbage, don't waste your time."
    ] * 10  # 100ê°œ ë¬¸ì¥ìœ¼ë¡œ í™•ì¥

    print(f"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìˆ˜: {len(test_texts)}ê°œ")

    # ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    single_times = []
    for i, text in enumerate(test_texts[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¸¡ì •
        start_time = time.time()

        inputs = tokenizer(text, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits.argmax(dim=-1)

        end_time = time.time()
        single_times.append(end_time - start_time)

        if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"   ë¬¸ì¥ {i+1}: {end_time - start_time:.4f}ì´ˆ")

    avg_single_time = sum(single_times) / len(single_times)
    print(f"   í‰ê·  ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡  ì‹œê°„: {avg_single_time:.4f}ì´ˆ")

    # ë°°ì¹˜ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    print(f"\nğŸ“Š ë°°ì¹˜ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:")
    batch_sizes = [1, 4, 8, 16, 32]

    for batch_size in batch_sizes:
        if batch_size > len(test_texts):
            continue

        batch_texts = test_texts[:batch_size]

        # ë°°ì¹˜ í† í°í™”
        start_time = time.time()
        inputs = tokenizer(batch_texts, return_tensors="pt",
                           padding=True, truncation=True).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits.argmax(dim=-1)

        end_time = time.time()
        total_time = end_time - start_time
        per_sample_time = total_time / batch_size

        print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:2d}: {
              total_time:.4f}ì´ˆ (ë¬¸ì¥ë‹¹ {per_sample_time:.4f}ì´ˆ)")


def benchmark_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)

    device = get_optimal_device()

    if device == "mps":
        # MPS ë©”ëª¨ë¦¬ ì •ë³´
        print("MPS ë©”ëª¨ë¦¬ ì •ë³´:")
        print(f"   ë””ë°”ì´ìŠ¤: {device}")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •ì„ ìœ„í•œ ë”ë¯¸ í…ì„œ ìƒì„±
        memory_usage = []
        tensor_sizes = [1000, 2000, 4000, 8000, 16000]

        for size in tensor_sizes:
            try:
                # ë”ë¯¸ í…ì„œ ìƒì„±
                dummy_tensor = torch.rand(size, size).to(device)
                memory_usage.append(size * size * 4)  # 4ë°”ì´íŠ¸ per float32
                print(f"   {size}x{size} í…ì„œ: {size*size*4/1024**2:.2f}MB")
            except RuntimeError as e:
                print(f"   {size}x{size} í…ì„œ: ë©”ëª¨ë¦¬ ë¶€ì¡± - {e}")
                break
    else:
        print("CPU ëª¨ë“œì—ì„œëŠ” ë©”ëª¨ë¦¬ ì¸¡ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")


def benchmark_data_loading():
    """ë°ì´í„° ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)

    # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
    sample_sizes = [100, 500, 1000, 5000]

    for sample_size in sample_sizes:
        print(f"\nìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ")

        start_time = time.time()

        # ë°ì´í„°ì…‹ ë¡œë“œ
        full_dataset = load_dataset("imdb", cache_dir="./data_cache")
        dataset = full_dataset["train"].select(range(sample_size))

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

        # í† í°í™”
        def tokenize(batch):
            return tokenizer(batch["text"], truncation=True, max_length=512)

        dataset = dataset.map(tokenize, batched=True)

        end_time = time.time()
        print(f"   ë¡œë”© + í† í°í™” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"   ë¬¸ì¥ë‹¹ í‰ê·  ì‹œê°„: {(end_time - start_time)/sample_size:.4f}ì´ˆ")


def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜"""
    print("ğŸš€ M4 Pro ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
    print("=" * 60)

    # 1. ëª¨ë¸ ë¡œë”© ì„±ëŠ¥
    model, tokenizer = benchmark_model_loading()

    # 2. ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
    device = get_optimal_device()
    benchmark_inference_speed(model, tokenizer, device)

    # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬
    benchmark_memory_usage()

    # 4. ë°ì´í„° ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    benchmark_data_loading()

    print("\n" + "=" * 60)
    print("ğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ:")
    print("   1. ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ì„œ GPU í™œìš©ë„ í–¥ìƒ")
    print("   2. Mixed Precision Training (fp16) ì‚¬ìš©")
    print("   3. DataLoaderì˜ num_workers ì¦ê°€")
    print("   4. Gradient Accumulationìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ")
    print("   5. M4 Proì˜ Neural Engine í™œìš©")


if __name__ == "__main__":
    main()
