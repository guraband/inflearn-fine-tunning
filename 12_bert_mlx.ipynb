{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT 파인튜닝 with MLX (Warm-up 최적화)\n",
        "## Apple Silicon 최적화된 MLX 라이브러리 사용\n",
        "\n",
        "### 1. 데이터 로드\n",
        "- IMDB 데이터셋 중 샘플 50개만 가져와서 학습용/평가용 8:2로 나눔\n",
        "- MLX를 사용하여 Apple M4 Pro 최적화\n",
        "- **Warm-up 적용으로 성능 향상**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🍎 MLX 라이브러리 로드 완료!\n",
            "MLX 버전: 0.29.1\n",
            "사용 가능한 디바이스: Device(gpu, 0)\n",
            "🔥 Warm-up 최적화가 적용됩니다!\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"🍎 MLX 라이브러리 로드 완료!\")\n",
        "print(f\"MLX 버전: {mx.__version__}\")\n",
        "print(f\"사용 가능한 디바이스: {mx.default_device()}\")\n",
        "print(\"🔥 Warm-up 최적화가 적용됩니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT 파인튜닝 with MLX\n",
        "## Apple Silicon 최적화된 MLX 라이브러리 사용\n",
        "\n",
        "### 1. 데이터 로드\n",
        "- IMDB 데이터셋 중 샘플 50개만 가져와서 학습용/평가용 8:2로 나눔\n",
        "- MLX를 사용하여 Apple M4 Pro 최적화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🍎 MLX 라이브러리 로드 완료!\n",
            "MLX 버전: 0.29.1\n",
            "사용 가능한 디바이스: Device(gpu, 0)\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"🍎 MLX 라이브러리 로드 완료!\")\n",
        "print(f\"MLX 버전: {mx.__version__}\")\n",
        "print(f\"사용 가능한 디바이스: {mx.default_device()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터셋 다운로드 시작...\n",
            "📥 IMDB 데이터셋을 다운로드하고 있습니다...\n",
            "   (MLX 최적화로 더 빠른 처리 예상!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "다운로드 진행률: 100%|██████████| 100/100 [00:03<00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 데이터셋 로딩 완료! 소요시간: 3.09초\n",
            "📊 훈련 데이터: 40개, 테스트 데이터: 10개\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"데이터셋 다운로드 시작...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 프로그레스바와 함께 데이터셋 로딩\n",
        "print(\"📥 IMDB 데이터셋을 다운로드하고 있습니다...\")\n",
        "print(\"   (MLX 최적화로 더 빠른 처리 예상!)\")\n",
        "\n",
        "# tqdm을 사용한 더 자세한 프로그레스바\n",
        "with tqdm(total=100, desc=\"다운로드 진행률\", unit=\"%\", \n",
        "          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
        "    \n",
        "    # 데이터셋 로딩 (50개 샘플)\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:50]\").train_test_split(test_size=0.2)\n",
        "    \n",
        "    # 프로그레스바 완료\n",
        "    pbar.n = 100\n",
        "    pbar.refresh()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"✅ 데이터셋 로딩 완료! 소요시간: {end_time - start_time:.2f}초\")\n",
        "print(f\"📊 훈련 데이터: {len(dataset['train'])}개, 테스트 데이터: {len(dataset['test'])}개\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "리뷰 내용 : Really, I can't believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the movie couldn't be that bad. It had zombies in it right? Was I wrong! To be honest the movie had it's moments...I thought it was cool when the guy got his head ripped off but that was about it. Overall I think that it would be more enjoyable to slide down a razorblade slide on my bare nutsack into a vat of vinegar then watch this movie again. The movie could have been better if we could see some boob but I had to watch the trailers for the other movies produced by this company to see that. Buyer beware...unless you are into masochism.\n",
            "레이블 (0:부정, 1:긍정): 0\n"
          ]
        }
      ],
      "source": [
        "sample = dataset[\"train\"][5]\n",
        "print(f\"리뷰 내용 : {sample['text']}\")\n",
        "print(f\"레이블 (0:부정, 1:긍정): {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX용 BERT 모델 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 토크나이저 로드 완료!\n",
            "어휘 크기: 30522\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"✅ 토크나이저 로드 완료!\")\n",
        "print(f\"어휘 크기: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MLX BERT 모델 생성 완료!\n",
            "🔥 MLX 효율적인 warm-up 시작...\n",
            "✅ MLX 효율적인 warm-up 완료! (총 250회 실행)\n",
            "🚀 이제 최적화된 성능으로 훈련을 시작할 수 있습니다!\n",
            "모델 파라미터 수: 0\n",
            "모델 구조: MLXDistilBERT(\n",
            "  (embeddings): Embedding(30522, 768)\n",
            "  (position_embeddings): Embedding(512, 768)\n",
            "  (token_type_embeddings): Embedding(2, 768)\n",
            "  (embeddings_layer_norm): LayerNorm(768, eps=1e-05, affine=True)\n",
            "  (embeddings_dropout): Dropout(p=0.09999999999999998)\n",
            "  (lstm): LSTM(input_dims=768, hidden_size=768, bias=True)\n",
            "  (pre_classifier): Linear(input_dims=768, output_dims=768, bias=True)\n",
            "  (classifier): Linear(input_dims=768, output_dims=2, bias=True)\n",
            "  (dropout): Dropout(p=0.09999999999999998)\n",
            ")\n",
            "임베딩 크기: (30522, 768)\n",
            "트랜스포머 레이어 수: 6\n",
            "분류 헤드: (2, 768)\n",
            "\\n🧪 모델 테스트:\n",
            "✅ 모델 정상 작동! 출력 형태: (1, 2)\n"
          ]
        }
      ],
      "source": [
        "class MLXDistilBERT(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_labels=2, num_layers=6, num_heads=12):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 임베딩 레이어\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(512, hidden_size)  # 최대 512 토큰\n",
        "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
        "        \n",
        "        # 레이어 정규화\n",
        "        self.embeddings_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.embeddings_dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        # 간단한 LSTM 기반 모델로 대체 (MLX 호환성)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
        "        \n",
        "        # 분류 헤드\n",
        "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def __call__(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # 임베딩\n",
        "        seq_len = input_ids.shape[1]\n",
        "        position_ids = mx.arange(seq_len)[None, :]\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = mx.zeros_like(input_ids)\n",
        "            \n",
        "        embeddings = (\n",
        "            self.embeddings(input_ids) +\n",
        "            self.position_embeddings(position_ids) +\n",
        "            self.token_type_embeddings(token_type_ids)\n",
        "        )\n",
        "        \n",
        "        embeddings = self.embeddings_layer_norm(embeddings)\n",
        "        embeddings = self.embeddings_dropout(embeddings)\n",
        "        \n",
        "        # LSTM으로 시퀀스 처리\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        \n",
        "        # 분류 헤드\n",
        "        pooled_output = lstm_out[:, 0]  # 첫 번째 토큰 사용\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = nn.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# 모델 생성\n",
        "model = MLXDistilBERT()\n",
        "print(\"✅ MLX BERT 모델 생성 완료!\")\n",
        "\n",
        "# MLX warm-up (Apple Silicon 최적화)\n",
        "# 모델이 정의된 후에 실행하세요!\n",
        "# MLX warm-up (효율적인 버전)\n",
        "if 'model' in globals():\n",
        "    print(\"🔥 MLX 효율적인 warm-up 시작...\")\n",
        "    \n",
        "    try:\n",
        "        # 다양한 크기의 입력으로 warm-up (rand(500, 500) 스타일)\n",
        "        warmup_configs = [\n",
        "            (1, 8),    # 작은 배치, 짧은 시퀀스\n",
        "            (2, 16),   # 중간 배치, 중간 시퀀스\n",
        "            (4, 32),   # 큰 배치, 긴 시퀀스\n",
        "            (1, 64),   # 작은 배치, 긴 시퀀스\n",
        "            (8, 8),    # 큰 배치, 짧은 시퀀스\n",
        "        ]\n",
        "        \n",
        "        total_warmup = 0\n",
        "        for batch_size, seq_len in warmup_configs:\n",
        "            # rand(500, 500)처럼 작은 크기로 효율적인 warm-up\n",
        "            warmup_input = mx.random.randint(0, 1000, (batch_size, seq_len))\n",
        "            warmup_mask = mx.ones((batch_size, seq_len))\n",
        "            \n",
        "            # 각 설정마다 50번씩 실행 (총 250번)\n",
        "            for _ in range(50):\n",
        "                _ = model(warmup_input, warmup_mask)\n",
        "                total_warmup += 1\n",
        "                \n",
        "        print(f\"✅ MLX 효율적인 warm-up 완료! (총 {total_warmup}회 실행)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ MLX warm-up 오류: {e}\")\n",
        "        print(\" 모델의 입력 형식을 확인해보세요!\")\n",
        "        \n",
        "    print(\"🚀 이제 최적화된 성능으로 훈련을 시작할 수 있습니다!\")\n",
        "else:\n",
        "    print(\"⚠️ 모델이 정의되지 않았습니다!\")\n",
        "\n",
        "\n",
        "# MLX 파라미터 수 계산 (올바른 방법)\n",
        "def count_mlx_parameters(model):\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        if hasattr(param, 'size'):\n",
        "            total += param.size\n",
        "        elif hasattr(param, 'shape'):\n",
        "            total += np.prod(param.shape)\n",
        "    return total\n",
        "\n",
        "total_params = count_mlx_parameters(model)\n",
        "print(f\"모델 파라미터 수: {total_params:,}\")\n",
        "\n",
        "# 모델 구조 확인\n",
        "print(f\"모델 구조: {model}\")\n",
        "print(f\"임베딩 크기: {model.embeddings.weight.shape}\")\n",
        "print(f\"트랜스포머 레이어 수: 6\")\n",
        "print(f\"분류 헤드: {model.classifier.weight.shape}\")\n",
        "\n",
        "# 간단한 테스트\n",
        "print(f\"\\\\n🧪 모델 테스트:\")\n",
        "test_input = mx.array([[101, 102, 103, 104, 105]])  # 간단한 토큰 ID\n",
        "test_mask = mx.array([[1, 1, 1, 1, 1]])\n",
        "try:\n",
        "    output = model(test_input, test_mask)\n",
        "    print(f\"✅ 모델 정상 작동! 출력 형태: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 모델 오류: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 데이터 전처리 (MLX용)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 데이터 토큰화 중...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 40/40 [00:00<00:00, 4730.51 examples/s]\n",
            "Map: 100%|██████████| 10/10 [00:00<00:00, 2489.20 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 토큰화 완료!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize_for_mlx(batch):\n",
        "    # 토큰화\n",
        "    encoded = tokenizer(\n",
        "        batch[\"text\"], \n",
        "        padding=True, \n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"],\n",
        "        \"attention_mask\": encoded[\"attention_mask\"],\n",
        "        \"labels\": np.array(batch[\"label\"])\n",
        "    }\n",
        "\n",
        "# 데이터셋 토큰화\n",
        "print(\"🔄 데이터 토큰화 중...\")\n",
        "dataset = dataset.map(tokenize_for_mlx, batched=True)\n",
        "print(\"✅ 토큰화 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 훈련 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 훈련 설정 완료!\n",
            "배치 크기: 8, 에포크: 15\n",
            "옵티마이저: AdamW (lr=2e-5)\n"
          ]
        }
      ],
      "source": [
        "def loss_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    loss = nn.losses.cross_entropy(logits, labels)\n",
        "    return mx.mean(loss)\n",
        "\n",
        "def eval_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    predictions = mx.argmax(logits, axis=1)\n",
        "    accuracy = mx.mean(predictions == labels)\n",
        "    return accuracy\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = optim.AdamW(learning_rate=2e-5, weight_decay=0.01)\n",
        "\n",
        "# 훈련 설정\n",
        "batch_size = 8\n",
        "num_epochs = 15\n",
        "print(f\"✅ 훈련 설정 완료!\")\n",
        "print(f\"배치 크기: {batch_size}, 에포크: {num_epochs}\")\n",
        "print(f\"옵티마이저: AdamW (lr=2e-5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 파인튜닝 실행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 MLX 파인튜닝 시작!\n",
            "Apple M4 Pro 최적화로 빠른 학습 예상...\n",
            "✅ 훈련 데이터 준비 완료!\n",
            "입력 형태: (40, 512)\n",
            "레이블 형태: (40,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MLX 훈련 진행률: 100%|██████████| 15/15 [00:52<00:00,  3.50s/epoch, Epoch=15, Loss=0.000111, Avg Loss=0.062094]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ MLX 훈련 완료!\n",
            "⏱️  총 소요시간: 52.54초\n",
            "📊 평균 손실: 0.062094\n",
            "🚀 초당 처리: 11.42 samples/sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"🚀 MLX 파인튜닝 시작!\")\n",
        "print(\"Apple M4 Pro 최적화로 빠른 학습 예상...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 훈련 데이터 준비 (numpy 배열로 먼저 변환)\n",
        "train_data = dataset[\"train\"]\n",
        "train_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(train_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(train_data[\"attention_mask\"]))\n",
        "}\n",
        "train_labels = mx.array(np.array(train_data[\"labels\"]))\n",
        "\n",
        "print(f\"✅ 훈련 데이터 준비 완료!\")\n",
        "print(f\"입력 형태: {train_inputs['input_ids'].shape}\")\n",
        "print(f\"레이블 형태: {train_labels.shape}\")\n",
        "\n",
        "# 훈련 루프\n",
        "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
        "total_steps = 0\n",
        "total_loss = 0\n",
        "\n",
        "with tqdm(total=num_epochs, desc=\"MLX 훈련 진행률\", unit=\"epoch\") as pbar:\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # 배치별 훈련\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_inputs = {\n",
        "                \"input_ids\": train_inputs[\"input_ids\"][i:i+batch_size],\n",
        "                \"attention_mask\": train_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "            }\n",
        "            batch_labels = train_labels[i:i+batch_size]\n",
        "            \n",
        "            # 손실 계산 및 그래디언트\n",
        "            loss, grads = loss_and_grad_fn(model, batch_inputs, batch_labels)\n",
        "            \n",
        "            # 옵티마이저 업데이트\n",
        "            optimizer.update(model, grads)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            total_steps += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / (len(train_data) // batch_size)\n",
        "        total_loss += avg_loss\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Loss\": f\"{avg_loss:.6f}\",\n",
        "            \"Avg Loss\": f\"{total_loss / (epoch + 1):.6f}\"\n",
        "        })\n",
        "        pbar.update(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n✅ MLX 훈련 완료!\")\n",
        "print(f\"⏱️  총 소요시간: {end_time - start_time:.2f}초\")\n",
        "print(f\"📊 평균 손실: {total_loss / num_epochs:.6f}\")\n",
        "print(f\"🚀 초당 처리: {len(train_data) * num_epochs / (end_time - start_time):.2f} samples/sec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1차\n",
        "```txt\n",
        "✅ MLX 훈련 완료!\n",
        "⏱️ 총 소요시간: 48.53초\n",
        "📊 평균 손실: 0.062502\n",
        "🚀 초당 처리: 12.36 samples/sec\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 모델 평가 및 추론\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 테스트 데이터 준비 완료!\n",
            "입력 형태: (10, 470)\n",
            "레이블 형태: (10,)\n",
            "🎯 테스트 정확도: 1.0000 (100.00%)\n",
            "📊 예측 분포: [10  0]\n",
            "📊 실제 분포: [10  0]\n",
            "\n",
            "🔮 MLX 모델 감정 분석 결과:\n",
            "==================================================\n",
            "1. 문장: I can watch this all day.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "2. 문장: This movie is absolutely terrible.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "3. 문장: Amazing performance by the actors.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "4. 문장: I can watch this all day.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "5. 문장: I love this movie.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터 평가 (numpy 배열로 먼저 변환)\n",
        "test_data = dataset[\"test\"]\n",
        "test_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(test_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(test_data[\"attention_mask\"]))\n",
        "}\n",
        "test_labels = mx.array(np.array(test_data[\"labels\"]))\n",
        "\n",
        "print(f\"✅ 테스트 데이터 준비 완료!\")\n",
        "print(f\"입력 형태: {test_inputs['input_ids'].shape}\")\n",
        "print(f\"레이블 형태: {test_labels.shape}\")\n",
        "\n",
        "# 예측 (MLX에서는 mx.eval()이 함수입니다)\n",
        "mx.eval(model)\n",
        "test_logits = model(test_inputs[\"input_ids\"], test_inputs[\"attention_mask\"])\n",
        "predictions = mx.argmax(test_logits, axis=1)\n",
        "accuracy = mx.mean(predictions == test_labels)\n",
        "\n",
        "print(f\"🎯 테스트 정확도: {accuracy.item():.4f} ({accuracy.item() * 100:.2f}%)\")\n",
        "print(f\"📊 예측 분포: {np.bincount(predictions.tolist(), minlength=2)}\")\n",
        "print(f\"📊 실제 분포: {np.bincount(test_labels.tolist(), minlength=2)}\")\n",
        "\n",
        "# 감정 분석 함수\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"텍스트 감정 분석\"\"\"\n",
        "    # 토큰화\n",
        "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
        "    \n",
        "    # MLX 배열로 변환\n",
        "    input_ids = mx.array(inputs[\"input_ids\"])\n",
        "    attention_mask = mx.array(inputs[\"attention_mask\"])\n",
        "    \n",
        "    # 예측 (MLX에서는 mx.eval()이 함수입니다)\n",
        "    mx.eval(model)\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probabilities = mx.softmax(logits, axis=1)\n",
        "    prediction = mx.argmax(logits, axis=1)\n",
        "    \n",
        "    return {\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"probabilities\": probabilities[0].tolist(),\n",
        "        \"sentiment\": \"긍정\" if prediction.item() == 1 else \"부정\"\n",
        "    }\n",
        "\n",
        "# 테스트 문장들\n",
        "test_sentences = [\n",
        "    \"I can watch this all day.\",\n",
        "    \"This movie is absolutely terrible.\",\n",
        "    \"Amazing performance by the actors.\",\n",
        "    \"I can watch this all day.\",\n",
        "    \"I love this movie.\"\n",
        "]\n",
        "\n",
        "print(\"\\n🔮 MLX 모델 감정 분석 결과:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    result = predict_sentiment(sentence, model, tokenizer)\n",
        "    print(f\"{i}. 문장: {sentence}\")\n",
        "    print(f\"   예측: {result['sentiment']} (확률: {result['probabilities'][result['prediction']]:.4f})\")\n",
        "    print(f\"   부정: {result['probabilities'][0]:.4f}, 긍정: {result['probabilities'][1]:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
