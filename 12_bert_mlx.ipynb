{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT íŒŒì¸íŠœë‹ with MLX (Warm-up ìµœì í™”)\n",
        "## Apple Silicon ìµœì í™”ëœ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "\n",
        "### 1. ë°ì´í„° ë¡œë“œ\n",
        "- IMDB ë°ì´í„°ì…‹ ì¤‘ ìƒ˜í”Œ 50ê°œë§Œ ê°€ì ¸ì™€ì„œ í•™ìŠµìš©/í‰ê°€ìš© 8:2ë¡œ ë‚˜ëˆ”\n",
        "- MLXë¥¼ ì‚¬ìš©í•˜ì—¬ Apple M4 Pro ìµœì í™”\n",
        "- **Warm-up ì ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n",
            "MLX ë²„ì „: 0.29.1\n",
            "ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: Device(gpu, 0)\n",
            "ğŸ”¥ Warm-up ìµœì í™”ê°€ ì ìš©ë©ë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"MLX ë²„ì „: {mx.__version__}\")\n",
        "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {mx.default_device()}\")\n",
        "print(\"ğŸ”¥ Warm-up ìµœì í™”ê°€ ì ìš©ë©ë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT íŒŒì¸íŠœë‹ with MLX\n",
        "## Apple Silicon ìµœì í™”ëœ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "\n",
        "### 1. ë°ì´í„° ë¡œë“œ\n",
        "- IMDB ë°ì´í„°ì…‹ ì¤‘ ìƒ˜í”Œ 50ê°œë§Œ ê°€ì ¸ì™€ì„œ í•™ìŠµìš©/í‰ê°€ìš© 8:2ë¡œ ë‚˜ëˆ”\n",
        "- MLXë¥¼ ì‚¬ìš©í•˜ì—¬ Apple M4 Pro ìµœì í™”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n",
            "MLX ë²„ì „: 0.29.1\n",
            "ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: Device(gpu, 0)\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"MLX ë²„ì „: {mx.__version__}\")\n",
        "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {mx.default_device()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n",
            "ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\n",
            "   (MLX ìµœì í™”ë¡œ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì˜ˆìƒ!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: 3.09ì´ˆ\n",
            "ğŸ“Š í›ˆë ¨ ë°ì´í„°: 40ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: 10ê°œ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# í”„ë¡œê·¸ë ˆìŠ¤ë°”ì™€ í•¨ê»˜ ë°ì´í„°ì…‹ ë¡œë”©\n",
        "print(\"ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n",
        "print(\"   (MLX ìµœì í™”ë¡œ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì˜ˆìƒ!)\")\n",
        "\n",
        "# tqdmì„ ì‚¬ìš©í•œ ë” ìì„¸í•œ í”„ë¡œê·¸ë ˆìŠ¤ë°”\n",
        "with tqdm(total=100, desc=\"ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ \", unit=\"%\", \n",
        "          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
        "    \n",
        "    # ë°ì´í„°ì…‹ ë¡œë”© (50ê°œ ìƒ˜í”Œ)\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:50]\").train_test_split(test_size=0.2)\n",
        "    \n",
        "    # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì™„ë£Œ\n",
        "    pbar.n = 100\n",
        "    pbar.refresh()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
        "print(f\"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(dataset['train'])}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(dataset['test'])}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¦¬ë·° ë‚´ìš© : Really, I can't believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the movie couldn't be that bad. It had zombies in it right? Was I wrong! To be honest the movie had it's moments...I thought it was cool when the guy got his head ripped off but that was about it. Overall I think that it would be more enjoyable to slide down a razorblade slide on my bare nutsack into a vat of vinegar then watch this movie again. The movie could have been better if we could see some boob but I had to watch the trailers for the other movies produced by this company to see that. Buyer beware...unless you are into masochism.\n",
            "ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): 0\n"
          ]
        }
      ],
      "source": [
        "sample = dataset[\"train\"][5]\n",
        "print(f\"ë¦¬ë·° ë‚´ìš© : {sample['text']}\")\n",
        "print(f\"ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLXìš© BERT ëª¨ë¸ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\n",
            "ì–´íœ˜ í¬ê¸°: 30522\n"
          ]
        }
      ],
      "source": [
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… MLX BERT ëª¨ë¸ ìƒì„± ì™„ë£Œ!\n",
            "ğŸ”¥ MLX íš¨ìœ¨ì ì¸ warm-up ì‹œì‘...\n",
            "âœ… MLX íš¨ìœ¨ì ì¸ warm-up ì™„ë£Œ! (ì´ 250íšŒ ì‹¤í–‰)\n",
            "ğŸš€ ì´ì œ ìµœì í™”ëœ ì„±ëŠ¥ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
            "ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: 0\n",
            "ëª¨ë¸ êµ¬ì¡°: MLXDistilBERT(\n",
            "  (embeddings): Embedding(30522, 768)\n",
            "  (position_embeddings): Embedding(512, 768)\n",
            "  (token_type_embeddings): Embedding(2, 768)\n",
            "  (embeddings_layer_norm): LayerNorm(768, eps=1e-05, affine=True)\n",
            "  (embeddings_dropout): Dropout(p=0.09999999999999998)\n",
            "  (lstm): LSTM(input_dims=768, hidden_size=768, bias=True)\n",
            "  (pre_classifier): Linear(input_dims=768, output_dims=768, bias=True)\n",
            "  (classifier): Linear(input_dims=768, output_dims=2, bias=True)\n",
            "  (dropout): Dropout(p=0.09999999999999998)\n",
            ")\n",
            "ì„ë² ë”© í¬ê¸°: (30522, 768)\n",
            "íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜: 6\n",
            "ë¶„ë¥˜ í—¤ë“œ: (2, 768)\n",
            "\\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\n",
            "âœ… ëª¨ë¸ ì •ìƒ ì‘ë™! ì¶œë ¥ í˜•íƒœ: (1, 2)\n"
          ]
        }
      ],
      "source": [
        "class MLXDistilBERT(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_labels=2, num_layers=6, num_heads=12):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ì„ë² ë”© ë ˆì´ì–´\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(512, hidden_size)  # ìµœëŒ€ 512 í† í°\n",
        "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
        "        \n",
        "        # ë ˆì´ì–´ ì •ê·œí™”\n",
        "        self.embeddings_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.embeddings_dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        # ê°„ë‹¨í•œ LSTM ê¸°ë°˜ ëª¨ë¸ë¡œ ëŒ€ì²´ (MLX í˜¸í™˜ì„±)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
        "        \n",
        "        # ë¶„ë¥˜ í—¤ë“œ\n",
        "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def __call__(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # ì„ë² ë”©\n",
        "        seq_len = input_ids.shape[1]\n",
        "        position_ids = mx.arange(seq_len)[None, :]\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = mx.zeros_like(input_ids)\n",
        "            \n",
        "        embeddings = (\n",
        "            self.embeddings(input_ids) +\n",
        "            self.position_embeddings(position_ids) +\n",
        "            self.token_type_embeddings(token_type_ids)\n",
        "        )\n",
        "        \n",
        "        embeddings = self.embeddings_layer_norm(embeddings)\n",
        "        embeddings = self.embeddings_dropout(embeddings)\n",
        "        \n",
        "        # LSTMìœ¼ë¡œ ì‹œí€€ìŠ¤ ì²˜ë¦¬\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        \n",
        "        # ë¶„ë¥˜ í—¤ë“œ\n",
        "        pooled_output = lstm_out[:, 0]  # ì²« ë²ˆì§¸ í† í° ì‚¬ìš©\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = nn.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "model = MLXDistilBERT()\n",
        "print(\"âœ… MLX BERT ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# MLX warm-up (Apple Silicon ìµœì í™”)\n",
        "# ëª¨ë¸ì´ ì •ì˜ëœ í›„ì— ì‹¤í–‰í•˜ì„¸ìš”!\n",
        "# MLX warm-up (íš¨ìœ¨ì ì¸ ë²„ì „)\n",
        "if 'model' in globals():\n",
        "    print(\"ğŸ”¥ MLX íš¨ìœ¨ì ì¸ warm-up ì‹œì‘...\")\n",
        "    \n",
        "    try:\n",
        "        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì…ë ¥ìœ¼ë¡œ warm-up (rand(500, 500) ìŠ¤íƒ€ì¼)\n",
        "        warmup_configs = [\n",
        "            (1, 8),    # ì‘ì€ ë°°ì¹˜, ì§§ì€ ì‹œí€€ìŠ¤\n",
        "            (2, 16),   # ì¤‘ê°„ ë°°ì¹˜, ì¤‘ê°„ ì‹œí€€ìŠ¤\n",
        "            (4, 32),   # í° ë°°ì¹˜, ê¸´ ì‹œí€€ìŠ¤\n",
        "            (1, 64),   # ì‘ì€ ë°°ì¹˜, ê¸´ ì‹œí€€ìŠ¤\n",
        "            (8, 8),    # í° ë°°ì¹˜, ì§§ì€ ì‹œí€€ìŠ¤\n",
        "        ]\n",
        "        \n",
        "        total_warmup = 0\n",
        "        for batch_size, seq_len in warmup_configs:\n",
        "            # rand(500, 500)ì²˜ëŸ¼ ì‘ì€ í¬ê¸°ë¡œ íš¨ìœ¨ì ì¸ warm-up\n",
        "            warmup_input = mx.random.randint(0, 1000, (batch_size, seq_len))\n",
        "            warmup_mask = mx.ones((batch_size, seq_len))\n",
        "            \n",
        "            # ê° ì„¤ì •ë§ˆë‹¤ 50ë²ˆì”© ì‹¤í–‰ (ì´ 250ë²ˆ)\n",
        "            for _ in range(50):\n",
        "                _ = model(warmup_input, warmup_mask)\n",
        "                total_warmup += 1\n",
        "                \n",
        "        print(f\"âœ… MLX íš¨ìœ¨ì ì¸ warm-up ì™„ë£Œ! (ì´ {total_warmup}íšŒ ì‹¤í–‰)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ MLX warm-up ì˜¤ë¥˜: {e}\")\n",
        "        print(\" ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ì„ í™•ì¸í•´ë³´ì„¸ìš”!\")\n",
        "        \n",
        "    print(\"ğŸš€ ì´ì œ ìµœì í™”ëœ ì„±ëŠ¥ìœ¼ë¡œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
        "else:\n",
        "    print(\"âš ï¸ ëª¨ë¸ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "\n",
        "# MLX íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (ì˜¬ë°”ë¥¸ ë°©ë²•)\n",
        "def count_mlx_parameters(model):\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        if hasattr(param, 'size'):\n",
        "            total += param.size\n",
        "        elif hasattr(param, 'shape'):\n",
        "            total += np.prod(param.shape)\n",
        "    return total\n",
        "\n",
        "total_params = count_mlx_parameters(model)\n",
        "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
        "print(f\"ëª¨ë¸ êµ¬ì¡°: {model}\")\n",
        "print(f\"ì„ë² ë”© í¬ê¸°: {model.embeddings.weight.shape}\")\n",
        "print(f\"íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜: 6\")\n",
        "print(f\"ë¶„ë¥˜ í—¤ë“œ: {model.classifier.weight.shape}\")\n",
        "\n",
        "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
        "print(f\"\\\\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\")\n",
        "test_input = mx.array([[101, 102, 103, 104, 105]])  # ê°„ë‹¨í•œ í† í° ID\n",
        "test_mask = mx.array([[1, 1, 1, 1, 1]])\n",
        "try:\n",
        "    output = model(test_input, test_mask)\n",
        "    print(f\"âœ… ëª¨ë¸ ì •ìƒ ì‘ë™! ì¶œë ¥ í˜•íƒœ: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ëª¨ë¸ ì˜¤ë¥˜: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ë°ì´í„° ì „ì²˜ë¦¬ (MLXìš©)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 4730.51 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2489.20 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í† í°í™” ì™„ë£Œ!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize_for_mlx(batch):\n",
        "    # í† í°í™”\n",
        "    encoded = tokenizer(\n",
        "        batch[\"text\"], \n",
        "        padding=True, \n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"],\n",
        "        \"attention_mask\": encoded[\"attention_mask\"],\n",
        "        \"labels\": np.array(batch[\"label\"])\n",
        "    }\n",
        "\n",
        "# ë°ì´í„°ì…‹ í† í°í™”\n",
        "print(\"ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...\")\n",
        "dataset = dataset.map(tokenize_for_mlx, batched=True)\n",
        "print(\"âœ… í† í°í™” ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX í›ˆë ¨ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\n",
            "ë°°ì¹˜ í¬ê¸°: 8, ì—í¬í¬: 15\n",
            "ì˜µí‹°ë§ˆì´ì €: AdamW (lr=2e-5)\n"
          ]
        }
      ],
      "source": [
        "def loss_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    loss = nn.losses.cross_entropy(logits, labels)\n",
        "    return mx.mean(loss)\n",
        "\n",
        "def eval_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    predictions = mx.argmax(logits, axis=1)\n",
        "    accuracy = mx.mean(predictions == labels)\n",
        "    return accuracy\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "optimizer = optim.AdamW(learning_rate=2e-5, weight_decay=0.01)\n",
        "\n",
        "# í›ˆë ¨ ì„¤ì •\n",
        "batch_size = 8\n",
        "num_epochs = 15\n",
        "print(f\"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}, ì—í¬í¬: {num_epochs}\")\n",
        "print(f\"ì˜µí‹°ë§ˆì´ì €: AdamW (lr=2e-5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX íŒŒì¸íŠœë‹ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ MLX íŒŒì¸íŠœë‹ ì‹œì‘!\n",
            "Apple M4 Pro ìµœì í™”ë¡œ ë¹ ë¥¸ í•™ìŠµ ì˜ˆìƒ...\n",
            "âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n",
            "ì…ë ¥ í˜•íƒœ: (40, 512)\n",
            "ë ˆì´ë¸” í˜•íƒœ: (40,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MLX í›ˆë ¨ ì§„í–‰ë¥ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:52<00:00,  3.50s/epoch, Epoch=15, Loss=0.000111, Avg Loss=0.062094]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… MLX í›ˆë ¨ ì™„ë£Œ!\n",
            "â±ï¸  ì´ ì†Œìš”ì‹œê°„: 52.54ì´ˆ\n",
            "ğŸ“Š í‰ê·  ì†ì‹¤: 0.062094\n",
            "ğŸš€ ì´ˆë‹¹ ì²˜ë¦¬: 11.42 samples/sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸš€ MLX íŒŒì¸íŠœë‹ ì‹œì‘!\")\n",
        "print(\"Apple M4 Pro ìµœì í™”ë¡œ ë¹ ë¥¸ í•™ìŠµ ì˜ˆìƒ...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (numpy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜)\n",
        "train_data = dataset[\"train\"]\n",
        "train_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(train_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(train_data[\"attention_mask\"]))\n",
        "}\n",
        "train_labels = mx.array(np.array(train_data[\"labels\"]))\n",
        "\n",
        "print(f\"âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "print(f\"ì…ë ¥ í˜•íƒœ: {train_inputs['input_ids'].shape}\")\n",
        "print(f\"ë ˆì´ë¸” í˜•íƒœ: {train_labels.shape}\")\n",
        "\n",
        "# í›ˆë ¨ ë£¨í”„\n",
        "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
        "total_steps = 0\n",
        "total_loss = 0\n",
        "\n",
        "with tqdm(total=num_epochs, desc=\"MLX í›ˆë ¨ ì§„í–‰ë¥ \", unit=\"epoch\") as pbar:\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # ë°°ì¹˜ë³„ í›ˆë ¨\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_inputs = {\n",
        "                \"input_ids\": train_inputs[\"input_ids\"][i:i+batch_size],\n",
        "                \"attention_mask\": train_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "            }\n",
        "            batch_labels = train_labels[i:i+batch_size]\n",
        "            \n",
        "            # ì†ì‹¤ ê³„ì‚° ë° ê·¸ë˜ë””ì–¸íŠ¸\n",
        "            loss, grads = loss_and_grad_fn(model, batch_inputs, batch_labels)\n",
        "            \n",
        "            # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸\n",
        "            optimizer.update(model, grads)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            total_steps += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / (len(train_data) // batch_size)\n",
        "        total_loss += avg_loss\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Loss\": f\"{avg_loss:.6f}\",\n",
        "            \"Avg Loss\": f\"{total_loss / (epoch + 1):.6f}\"\n",
        "        })\n",
        "        pbar.update(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nâœ… MLX í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
        "print(f\"ğŸ“Š í‰ê·  ì†ì‹¤: {total_loss / num_epochs:.6f}\")\n",
        "print(f\"ğŸš€ ì´ˆë‹¹ ì²˜ë¦¬: {len(train_data) * num_epochs / (end_time - start_time):.2f} samples/sec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1ì°¨\n",
        "```txt\n",
        "âœ… MLX í›ˆë ¨ ì™„ë£Œ!\n",
        "â±ï¸ ì´ ì†Œìš”ì‹œê°„: 48.53ì´ˆ\n",
        "ğŸ“Š í‰ê·  ì†ì‹¤: 0.062502\n",
        "ğŸš€ ì´ˆë‹¹ ì²˜ë¦¬: 12.36 samples/sec\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n",
            "ì…ë ¥ í˜•íƒœ: (10, 470)\n",
            "ë ˆì´ë¸” í˜•íƒœ: (10,)\n",
            "ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: 1.0000 (100.00%)\n",
            "ğŸ“Š ì˜ˆì¸¡ ë¶„í¬: [10  0]\n",
            "ğŸ“Š ì‹¤ì œ ë¶„í¬: [10  0]\n",
            "\n",
            "ğŸ”® MLX ëª¨ë¸ ê°ì • ë¶„ì„ ê²°ê³¼:\n",
            "==================================================\n",
            "1. ë¬¸ì¥: I can watch this all day.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "2. ë¬¸ì¥: This movie is absolutely terrible.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "3. ë¬¸ì¥: Amazing performance by the actors.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "4. ë¬¸ì¥: I can watch this all day.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "5. ë¬¸ì¥: I love this movie.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ (numpy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜)\n",
        "test_data = dataset[\"test\"]\n",
        "test_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(test_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(test_data[\"attention_mask\"]))\n",
        "}\n",
        "test_labels = mx.array(np.array(test_data[\"labels\"]))\n",
        "\n",
        "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "print(f\"ì…ë ¥ í˜•íƒœ: {test_inputs['input_ids'].shape}\")\n",
        "print(f\"ë ˆì´ë¸” í˜•íƒœ: {test_labels.shape}\")\n",
        "\n",
        "# ì˜ˆì¸¡ (MLXì—ì„œëŠ” mx.eval()ì´ í•¨ìˆ˜ì…ë‹ˆë‹¤)\n",
        "mx.eval(model)\n",
        "test_logits = model(test_inputs[\"input_ids\"], test_inputs[\"attention_mask\"])\n",
        "predictions = mx.argmax(test_logits, axis=1)\n",
        "accuracy = mx.mean(predictions == test_labels)\n",
        "\n",
        "print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy.item():.4f} ({accuracy.item() * 100:.2f}%)\")\n",
        "print(f\"ğŸ“Š ì˜ˆì¸¡ ë¶„í¬: {np.bincount(predictions.tolist(), minlength=2)}\")\n",
        "print(f\"ğŸ“Š ì‹¤ì œ ë¶„í¬: {np.bincount(test_labels.tolist(), minlength=2)}\")\n",
        "\n",
        "# ê°ì • ë¶„ì„ í•¨ìˆ˜\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„\"\"\"\n",
        "    # í† í°í™”\n",
        "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
        "    \n",
        "    # MLX ë°°ì—´ë¡œ ë³€í™˜\n",
        "    input_ids = mx.array(inputs[\"input_ids\"])\n",
        "    attention_mask = mx.array(inputs[\"attention_mask\"])\n",
        "    \n",
        "    # ì˜ˆì¸¡ (MLXì—ì„œëŠ” mx.eval()ì´ í•¨ìˆ˜ì…ë‹ˆë‹¤)\n",
        "    mx.eval(model)\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probabilities = mx.softmax(logits, axis=1)\n",
        "    prediction = mx.argmax(logits, axis=1)\n",
        "    \n",
        "    return {\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"probabilities\": probabilities[0].tolist(),\n",
        "        \"sentiment\": \"ê¸ì •\" if prediction.item() == 1 else \"ë¶€ì •\"\n",
        "    }\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "test_sentences = [\n",
        "    \"I can watch this all day.\",\n",
        "    \"This movie is absolutely terrible.\",\n",
        "    \"Amazing performance by the actors.\",\n",
        "    \"I can watch this all day.\",\n",
        "    \"I love this movie.\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ”® MLX ëª¨ë¸ ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    result = predict_sentiment(sentence, model, tokenizer)\n",
        "    print(f\"{i}. ë¬¸ì¥: {sentence}\")\n",
        "    print(f\"   ì˜ˆì¸¡: {result['sentiment']} (í™•ë¥ : {result['probabilities'][result['prediction']]:.4f})\")\n",
        "    print(f\"   ë¶€ì •: {result['probabilities'][0]:.4f}, ê¸ì •: {result['probabilities'][1]:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
