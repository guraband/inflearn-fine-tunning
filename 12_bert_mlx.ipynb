{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT íŒŒì¸íŠœë‹ with MLX\n",
        "## Apple Silicon ìµœì í™”ëœ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "\n",
        "### 1. ë°ì´í„° ë¡œë“œ\n",
        "- IMDB ë°ì´í„°ì…‹ ì¤‘ ìƒ˜í”Œ 50ê°œë§Œ ê°€ì ¸ì™€ì„œ í•™ìŠµìš©/í‰ê°€ìš© 8:2ë¡œ ë‚˜ëˆ”\n",
        "- MLXë¥¼ ì‚¬ìš©í•˜ì—¬ Apple M4 Pro ìµœì í™”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n",
            "MLX ë²„ì „: 0.29.1\n",
            "ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: Device(gpu, 0)\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"ğŸ MLX ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"MLX ë²„ì „: {mx.__version__}\")\n",
        "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {mx.default_device()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\n",
            "ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\n",
            "   (MLX ìµœì í™”ë¡œ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì˜ˆìƒ!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: 2.70ì´ˆ\n",
            "ğŸ“Š í›ˆë ¨ ë°ì´í„°: 40ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: 10ê°œ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# í”„ë¡œê·¸ë ˆìŠ¤ë°”ì™€ í•¨ê»˜ ë°ì´í„°ì…‹ ë¡œë”©\n",
        "print(\"ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n",
        "print(\"   (MLX ìµœì í™”ë¡œ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì˜ˆìƒ!)\")\n",
        "\n",
        "# tqdmì„ ì‚¬ìš©í•œ ë” ìì„¸í•œ í”„ë¡œê·¸ë ˆìŠ¤ë°”\n",
        "with tqdm(total=100, desc=\"ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ \", unit=\"%\", \n",
        "          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
        "    \n",
        "    # ë°ì´í„°ì…‹ ë¡œë”© (50ê°œ ìƒ˜í”Œ)\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:50]\").train_test_split(test_size=0.2)\n",
        "    \n",
        "    # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì™„ë£Œ\n",
        "    pbar.n = 100\n",
        "    pbar.refresh()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
        "print(f\"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(dataset['train'])}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(dataset['test'])}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¦¬ë·° ë‚´ìš© : I have this film out of the library right now and I haven't finished watching it. It is so bad I am in disbelief. Audrey Hepburn had totally lost her talent by then, although she'd pretty much finished with it in 'Robin and Marian.' This is the worst thing about this appallingly stupid film. It's really only of interest because it was her last feature film and because of the Dorothy Stratten appearance just prior to her homicide.<br /><br />There is nothing but idiocy between Gazzara and his cronies. Little signals and little bows and nods to real screwball comedy of which this is the faintest, palest shadow.<br /><br />Who could believe that there are even some of the same Manhattan environs that Hepburn inhabited so magically and even mythically in 'Breakfast at Tiffany's' twenty years earlier? The soundtrack of old Sinatra songs and the Gershwin song from which the title is taken is too loud and obvious--you sure don't have to wait for the credits to find out that something was subtly woven into the cine-musique of the picture to know when the songs blasted out at you.<br /><br />'Reverting to type' means going back up as well as going back down, I guess. In this case, Audrey Hepburn's chic European lady is all you see of someone who was formerly occasionally an actress and always a star. Here she has even lost her talent as a star. If someone whose talent was continuing to grow in the period, like Ann-Margret, had played the role, there would have been some life in it, even given the unbelievably bad material and Mongoloid-level situations.<br /><br />Hepburn was a great person, of course, greater than most movie stars ever dreamed of being, and she was once one of the most charming and beautiful of film actors. After this dreadful performance, she went on to make an atrocious TV movie with Robert Wagner called 'Love Among Thieves.' In 'They all Laughed' it is as though she were still playing an ingenue in her 50's. Even much vainer and obviously less intelligent actresses who insisted upon doing this like Lana Turner were infinitely more effective than is Hepburn. Turner took acting seriously even when she was bad. Hepburn doesn't take it seriously at all, couldn't be bothered with it; even her hair and clothes look tacky. Her last really good work was in 'Two for the Road,' perhaps her most perfect, if possibly not her best in many ways.<br /><br />And that girl who plays the country singer is just sickening. John Ritter is horrible, there is simply nothing to recommend this film except to see Dorothy Stratten, who was truly pretty. Otherwise, critic David Thomson's oft-used phrase 'losing his/her talent' never has made more sense.<br /><br />Ben Gazarra had lost all sex appeal by then, and so we have 2 films with Gazarra and Hepburn--who could ask for anything less? Sandra Dee's last, pitiful film 'Lost,' from 2 years later, a low-budget nothing, had more to it than this. At least Ms. Dee spoke in her own voice; by 1981, Audrey Hepburn's accent just sounded silly; she'd go on to do the PBS 'Gardens of the World with Audrey Hepburn' and there her somewhat irritating accent works as she walks through English gardens with aristocrats or waxes effusively about 'what I like most is when flowers go back to nature!' as in naturalized daffodils, but in an actual fictional movie, she just sounds ridiculous.<br /><br />To think that 'Breakfast at Tiffany's' was such a profound sort of light poetic thing with Audrey Hepburn one of the most beautiful women in the world--she was surely one of the most beautiful screen presences in 'My Fair Lady', matching Garbo in several things and Delphine Seyrig in 'Last Year at Marienbad.' And then this! And her final brief role as the angel 'Hap' in the Spielberg film 'Always' was just more of the lady stuff--corny, witless and stifling.<br /><br />I went to her memorial service at the Fifth Avenue Presbyterian Church, a beautiful service which included a boys' choir singing the Shaker hymn 'Simple Gifts.' The only thing not listed in the program was the sudden playing of Hepburn's singing 'Moon River' on the fire escape in 'Breakfast at Tiffany's,' and this brought much emotion and some real tears out in the congregation.<br /><br />A great lady who was once a fine actress (as in 'The Nun's Story') and one of the greatest and most beautiful of film stars in many movies of the 50's and 60's who became a truly bad one--that's not all that common. And perhaps it is only a great human being who, in making such things as film performances trivial, nevertheless has the largeness of mind to want to have the flaws pointed out mercilessly--which all of her late film work contained in abundance. Most of the talk about Hepburn's miscasting is about 'My Fair Lady.' But the one that should have had the original actress in it was 'Wait Until Dark,' which had starred Lee Remick on Broadway. Never as celebrated as Hepburn, she was a better actress in many ways (Hepburn was completely incapable of playing anything really sordid), although Hepburn was at least adequate enough in that part. After that, all of her acting went downhill.\n",
            "ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): 0\n"
          ]
        }
      ],
      "source": [
        "sample = dataset[\"train\"][5]\n",
        "print(f\"ë¦¬ë·° ë‚´ìš© : {sample['text']}\")\n",
        "print(f\"ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLXìš© BERT ëª¨ë¸ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\n",
            "ì–´íœ˜ í¬ê¸°: 30522\n"
          ]
        }
      ],
      "source": [
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… MLX BERT ëª¨ë¸ ìƒì„± ì™„ë£Œ!\n",
            "ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: 0\n",
            "ëª¨ë¸ êµ¬ì¡°: MLXDistilBERT(\n",
            "  (embeddings): Embedding(30522, 768)\n",
            "  (position_embeddings): Embedding(512, 768)\n",
            "  (token_type_embeddings): Embedding(2, 768)\n",
            "  (embeddings_layer_norm): LayerNorm(768, eps=1e-05, affine=True)\n",
            "  (embeddings_dropout): Dropout(p=0.09999999999999998)\n",
            "  (lstm): LSTM(input_dims=768, hidden_size=768, bias=True)\n",
            "  (pre_classifier): Linear(input_dims=768, output_dims=768, bias=True)\n",
            "  (classifier): Linear(input_dims=768, output_dims=2, bias=True)\n",
            "  (dropout): Dropout(p=0.09999999999999998)\n",
            ")\n",
            "ì„ë² ë”© í¬ê¸°: (30522, 768)\n",
            "íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜: 6\n",
            "ë¶„ë¥˜ í—¤ë“œ: (2, 768)\n",
            "\\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\n",
            "âœ… ëª¨ë¸ ì •ìƒ ì‘ë™! ì¶œë ¥ í˜•íƒœ: (1, 2)\n"
          ]
        }
      ],
      "source": [
        "class MLXDistilBERT(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_labels=2, num_layers=6, num_heads=12):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ì„ë² ë”© ë ˆì´ì–´\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(512, hidden_size)  # ìµœëŒ€ 512 í† í°\n",
        "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
        "        \n",
        "        # ë ˆì´ì–´ ì •ê·œí™”\n",
        "        self.embeddings_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.embeddings_dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        # ê°„ë‹¨í•œ LSTM ê¸°ë°˜ ëª¨ë¸ë¡œ ëŒ€ì²´ (MLX í˜¸í™˜ì„±)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
        "        \n",
        "        # ë¶„ë¥˜ í—¤ë“œ\n",
        "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def __call__(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # ì„ë² ë”©\n",
        "        seq_len = input_ids.shape[1]\n",
        "        position_ids = mx.arange(seq_len)[None, :]\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = mx.zeros_like(input_ids)\n",
        "            \n",
        "        embeddings = (\n",
        "            self.embeddings(input_ids) +\n",
        "            self.position_embeddings(position_ids) +\n",
        "            self.token_type_embeddings(token_type_ids)\n",
        "        )\n",
        "        \n",
        "        embeddings = self.embeddings_layer_norm(embeddings)\n",
        "        embeddings = self.embeddings_dropout(embeddings)\n",
        "        \n",
        "        # LSTMìœ¼ë¡œ ì‹œí€€ìŠ¤ ì²˜ë¦¬\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        \n",
        "        # ë¶„ë¥˜ í—¤ë“œ\n",
        "        pooled_output = lstm_out[:, 0]  # ì²« ë²ˆì§¸ í† í° ì‚¬ìš©\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = nn.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "model = MLXDistilBERT()\n",
        "print(\"âœ… MLX BERT ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# MLX íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (ì˜¬ë°”ë¥¸ ë°©ë²•)\n",
        "def count_mlx_parameters(model):\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        if hasattr(param, 'size'):\n",
        "            total += param.size\n",
        "        elif hasattr(param, 'shape'):\n",
        "            total += np.prod(param.shape)\n",
        "    return total\n",
        "\n",
        "total_params = count_mlx_parameters(model)\n",
        "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
        "print(f\"ëª¨ë¸ êµ¬ì¡°: {model}\")\n",
        "print(f\"ì„ë² ë”© í¬ê¸°: {model.embeddings.weight.shape}\")\n",
        "print(f\"íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜: 6\")\n",
        "print(f\"ë¶„ë¥˜ í—¤ë“œ: {model.classifier.weight.shape}\")\n",
        "\n",
        "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
        "print(f\"\\\\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\")\n",
        "test_input = mx.array([[101, 102, 103, 104, 105]])  # ê°„ë‹¨í•œ í† í° ID\n",
        "test_mask = mx.array([[1, 1, 1, 1, 1]])\n",
        "try:\n",
        "    output = model(test_input, test_mask)\n",
        "    print(f\"âœ… ëª¨ë¸ ì •ìƒ ì‘ë™! ì¶œë ¥ í˜•íƒœ: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ëª¨ë¸ ì˜¤ë¥˜: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ë°ì´í„° ì „ì²˜ë¦¬ (MLXìš©)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 4449.84 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2333.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í† í°í™” ì™„ë£Œ!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize_for_mlx(batch):\n",
        "    # í† í°í™”\n",
        "    encoded = tokenizer(\n",
        "        batch[\"text\"], \n",
        "        padding=True, \n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"],\n",
        "        \"attention_mask\": encoded[\"attention_mask\"],\n",
        "        \"labels\": np.array(batch[\"label\"])\n",
        "    }\n",
        "\n",
        "# ë°ì´í„°ì…‹ í† í°í™”\n",
        "print(\"ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...\")\n",
        "dataset = dataset.map(tokenize_for_mlx, batched=True)\n",
        "print(\"âœ… í† í°í™” ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX í›ˆë ¨ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\n",
            "ë°°ì¹˜ í¬ê¸°: 8, ì—í¬í¬: 15\n",
            "ì˜µí‹°ë§ˆì´ì €: AdamW (lr=2e-5)\n"
          ]
        }
      ],
      "source": [
        "def loss_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    loss = nn.losses.cross_entropy(logits, labels)\n",
        "    return mx.mean(loss)\n",
        "\n",
        "def eval_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    predictions = mx.argmax(logits, axis=1)\n",
        "    accuracy = mx.mean(predictions == labels)\n",
        "    return accuracy\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "optimizer = optim.AdamW(learning_rate=2e-5, weight_decay=0.01)\n",
        "\n",
        "# í›ˆë ¨ ì„¤ì •\n",
        "batch_size = 8\n",
        "num_epochs = 15\n",
        "print(f\"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}, ì—í¬í¬: {num_epochs}\")\n",
        "print(f\"ì˜µí‹°ë§ˆì´ì €: AdamW (lr=2e-5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX íŒŒì¸íŠœë‹ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ MLX íŒŒì¸íŠœë‹ ì‹œì‘!\n",
            "Apple M4 Pro ìµœì í™”ë¡œ ë¹ ë¥¸ í•™ìŠµ ì˜ˆìƒ...\n",
            "âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n",
            "ì…ë ¥ í˜•íƒœ: (40, 512)\n",
            "ë ˆì´ë¸” í˜•íƒœ: (40,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MLX í›ˆë ¨ ì§„í–‰ë¥ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:48<00:00,  3.23s/epoch, Epoch=15, Loss=0.000122, Avg Loss=0.062502]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… MLX í›ˆë ¨ ì™„ë£Œ!\n",
            "â±ï¸  ì´ ì†Œìš”ì‹œê°„: 48.53ì´ˆ\n",
            "ğŸ“Š í‰ê·  ì†ì‹¤: 0.062502\n",
            "ğŸš€ ì´ˆë‹¹ ì²˜ë¦¬: 12.36 samples/sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸš€ MLX íŒŒì¸íŠœë‹ ì‹œì‘!\")\n",
        "print(\"Apple M4 Pro ìµœì í™”ë¡œ ë¹ ë¥¸ í•™ìŠµ ì˜ˆìƒ...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (numpy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜)\n",
        "train_data = dataset[\"train\"]\n",
        "train_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(train_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(train_data[\"attention_mask\"]))\n",
        "}\n",
        "train_labels = mx.array(np.array(train_data[\"labels\"]))\n",
        "\n",
        "print(f\"âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "print(f\"ì…ë ¥ í˜•íƒœ: {train_inputs['input_ids'].shape}\")\n",
        "print(f\"ë ˆì´ë¸” í˜•íƒœ: {train_labels.shape}\")\n",
        "\n",
        "# í›ˆë ¨ ë£¨í”„\n",
        "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
        "total_steps = 0\n",
        "total_loss = 0\n",
        "\n",
        "with tqdm(total=num_epochs, desc=\"MLX í›ˆë ¨ ì§„í–‰ë¥ \", unit=\"epoch\") as pbar:\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # ë°°ì¹˜ë³„ í›ˆë ¨\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_inputs = {\n",
        "                \"input_ids\": train_inputs[\"input_ids\"][i:i+batch_size],\n",
        "                \"attention_mask\": train_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "            }\n",
        "            batch_labels = train_labels[i:i+batch_size]\n",
        "            \n",
        "            # ì†ì‹¤ ê³„ì‚° ë° ê·¸ë˜ë””ì–¸íŠ¸\n",
        "            loss, grads = loss_and_grad_fn(model, batch_inputs, batch_labels)\n",
        "            \n",
        "            # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸\n",
        "            optimizer.update(model, grads)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            total_steps += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / (len(train_data) // batch_size)\n",
        "        total_loss += avg_loss\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Loss\": f\"{avg_loss:.6f}\",\n",
        "            \"Avg Loss\": f\"{total_loss / (epoch + 1):.6f}\"\n",
        "        })\n",
        "        pbar.update(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nâœ… MLX í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
        "print(f\"ğŸ“Š í‰ê·  ì†ì‹¤: {total_loss / num_epochs:.6f}\")\n",
        "print(f\"ğŸš€ ì´ˆë‹¹ ì²˜ë¦¬: {len(train_data) * num_epochs / (end_time - start_time):.2f} samples/sec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\n",
            "ì…ë ¥ í˜•íƒœ: (10, 512)\n",
            "ë ˆì´ë¸” í˜•íƒœ: (10,)\n",
            "ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: 1.0000 (100.00%)\n",
            "ğŸ“Š ì˜ˆì¸¡ ë¶„í¬: [10  0]\n",
            "ğŸ“Š ì‹¤ì œ ë¶„í¬: [10  0]\n",
            "\n",
            "ğŸ”® MLX ëª¨ë¸ ê°ì • ë¶„ì„ ê²°ê³¼:\n",
            "==================================================\n",
            "1. ë¬¸ì¥: I can watch this all day.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9998)\n",
            "   ë¶€ì •: 0.9998, ê¸ì •: 0.0002\n",
            "\n",
            "2. ë¬¸ì¥: This movie is absolutely terrible.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "3. ë¬¸ì¥: Amazing performance by the actors.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "4. ë¬¸ì¥: I can watch this all day.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n",
            "5. ë¬¸ì¥: I love this movie.\n",
            "   ì˜ˆì¸¡: ë¶€ì • (í™•ë¥ : 0.9999)\n",
            "   ë¶€ì •: 0.9999, ê¸ì •: 0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ (numpy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜)\n",
        "test_data = dataset[\"test\"]\n",
        "test_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(test_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(test_data[\"attention_mask\"]))\n",
        "}\n",
        "test_labels = mx.array(np.array(test_data[\"labels\"]))\n",
        "\n",
        "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "print(f\"ì…ë ¥ í˜•íƒœ: {test_inputs['input_ids'].shape}\")\n",
        "print(f\"ë ˆì´ë¸” í˜•íƒœ: {test_labels.shape}\")\n",
        "\n",
        "# ì˜ˆì¸¡ (MLXì—ì„œëŠ” mx.eval()ì´ í•¨ìˆ˜ì…ë‹ˆë‹¤)\n",
        "mx.eval(model)\n",
        "test_logits = model(test_inputs[\"input_ids\"], test_inputs[\"attention_mask\"])\n",
        "predictions = mx.argmax(test_logits, axis=1)\n",
        "accuracy = mx.mean(predictions == test_labels)\n",
        "\n",
        "print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy.item():.4f} ({accuracy.item() * 100:.2f}%)\")\n",
        "print(f\"ğŸ“Š ì˜ˆì¸¡ ë¶„í¬: {np.bincount(predictions.tolist(), minlength=2)}\")\n",
        "print(f\"ğŸ“Š ì‹¤ì œ ë¶„í¬: {np.bincount(test_labels.tolist(), minlength=2)}\")\n",
        "\n",
        "# ê°ì • ë¶„ì„ í•¨ìˆ˜\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„\"\"\"\n",
        "    # í† í°í™”\n",
        "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
        "    \n",
        "    # MLX ë°°ì—´ë¡œ ë³€í™˜\n",
        "    input_ids = mx.array(inputs[\"input_ids\"])\n",
        "    attention_mask = mx.array(inputs[\"attention_mask\"])\n",
        "    \n",
        "    # ì˜ˆì¸¡ (MLXì—ì„œëŠ” mx.eval()ì´ í•¨ìˆ˜ì…ë‹ˆë‹¤)\n",
        "    mx.eval(model)\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probabilities = mx.softmax(logits, axis=1)\n",
        "    prediction = mx.argmax(logits, axis=1)\n",
        "    \n",
        "    return {\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"probabilities\": probabilities[0].tolist(),\n",
        "        \"sentiment\": \"ê¸ì •\" if prediction.item() == 1 else \"ë¶€ì •\"\n",
        "    }\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "test_sentences = [\n",
        "    \"I can watch this all day.\",\n",
        "    \"This movie is absolutely terrible.\",\n",
        "    \"Amazing performance by the actors.\",\n",
        "    \"I can watch this all day.\",\n",
        "    \"I love this movie.\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ”® MLX ëª¨ë¸ ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    result = predict_sentiment(sentence, model, tokenizer)\n",
        "    print(f\"{i}. ë¬¸ì¥: {sentence}\")\n",
        "    print(f\"   ì˜ˆì¸¡: {result['sentiment']} (í™•ë¥ : {result['probabilities'][result['prediction']]:.4f})\")\n",
        "    print(f\"   ë¶€ì •: {result['probabilities'][0]:.4f}, ê¸ì •: {result['probabilities'][1]:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
