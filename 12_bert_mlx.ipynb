{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT 파인튜닝 with MLX\n",
        "## Apple Silicon 최적화된 MLX 라이브러리 사용\n",
        "\n",
        "### 1. 데이터 로드\n",
        "- IMDB 데이터셋 중 샘플 50개만 가져와서 학습용/평가용 8:2로 나눔\n",
        "- MLX를 사용하여 Apple M4 Pro 최적화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🍎 MLX 라이브러리 로드 완료!\n",
            "MLX 버전: 0.29.1\n",
            "사용 가능한 디바이스: Device(gpu, 0)\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"🍎 MLX 라이브러리 로드 완료!\")\n",
        "print(f\"MLX 버전: {mx.__version__}\")\n",
        "print(f\"사용 가능한 디바이스: {mx.default_device()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터셋 다운로드 시작...\n",
            "📥 IMDB 데이터셋을 다운로드하고 있습니다...\n",
            "   (MLX 최적화로 더 빠른 처리 예상!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "다운로드 진행률: 100%|██████████| 100/100 [00:02<00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 데이터셋 로딩 완료! 소요시간: 2.70초\n",
            "📊 훈련 데이터: 40개, 테스트 데이터: 10개\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"데이터셋 다운로드 시작...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 프로그레스바와 함께 데이터셋 로딩\n",
        "print(\"📥 IMDB 데이터셋을 다운로드하고 있습니다...\")\n",
        "print(\"   (MLX 최적화로 더 빠른 처리 예상!)\")\n",
        "\n",
        "# tqdm을 사용한 더 자세한 프로그레스바\n",
        "with tqdm(total=100, desc=\"다운로드 진행률\", unit=\"%\", \n",
        "          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
        "    \n",
        "    # 데이터셋 로딩 (50개 샘플)\n",
        "    dataset = load_dataset(\"imdb\", split=\"train[:50]\").train_test_split(test_size=0.2)\n",
        "    \n",
        "    # 프로그레스바 완료\n",
        "    pbar.n = 100\n",
        "    pbar.refresh()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"✅ 데이터셋 로딩 완료! 소요시간: {end_time - start_time:.2f}초\")\n",
        "print(f\"📊 훈련 데이터: {len(dataset['train'])}개, 테스트 데이터: {len(dataset['test'])}개\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "리뷰 내용 : I have this film out of the library right now and I haven't finished watching it. It is so bad I am in disbelief. Audrey Hepburn had totally lost her talent by then, although she'd pretty much finished with it in 'Robin and Marian.' This is the worst thing about this appallingly stupid film. It's really only of interest because it was her last feature film and because of the Dorothy Stratten appearance just prior to her homicide.<br /><br />There is nothing but idiocy between Gazzara and his cronies. Little signals and little bows and nods to real screwball comedy of which this is the faintest, palest shadow.<br /><br />Who could believe that there are even some of the same Manhattan environs that Hepburn inhabited so magically and even mythically in 'Breakfast at Tiffany's' twenty years earlier? The soundtrack of old Sinatra songs and the Gershwin song from which the title is taken is too loud and obvious--you sure don't have to wait for the credits to find out that something was subtly woven into the cine-musique of the picture to know when the songs blasted out at you.<br /><br />'Reverting to type' means going back up as well as going back down, I guess. In this case, Audrey Hepburn's chic European lady is all you see of someone who was formerly occasionally an actress and always a star. Here she has even lost her talent as a star. If someone whose talent was continuing to grow in the period, like Ann-Margret, had played the role, there would have been some life in it, even given the unbelievably bad material and Mongoloid-level situations.<br /><br />Hepburn was a great person, of course, greater than most movie stars ever dreamed of being, and she was once one of the most charming and beautiful of film actors. After this dreadful performance, she went on to make an atrocious TV movie with Robert Wagner called 'Love Among Thieves.' In 'They all Laughed' it is as though she were still playing an ingenue in her 50's. Even much vainer and obviously less intelligent actresses who insisted upon doing this like Lana Turner were infinitely more effective than is Hepburn. Turner took acting seriously even when she was bad. Hepburn doesn't take it seriously at all, couldn't be bothered with it; even her hair and clothes look tacky. Her last really good work was in 'Two for the Road,' perhaps her most perfect, if possibly not her best in many ways.<br /><br />And that girl who plays the country singer is just sickening. John Ritter is horrible, there is simply nothing to recommend this film except to see Dorothy Stratten, who was truly pretty. Otherwise, critic David Thomson's oft-used phrase 'losing his/her talent' never has made more sense.<br /><br />Ben Gazarra had lost all sex appeal by then, and so we have 2 films with Gazarra and Hepburn--who could ask for anything less? Sandra Dee's last, pitiful film 'Lost,' from 2 years later, a low-budget nothing, had more to it than this. At least Ms. Dee spoke in her own voice; by 1981, Audrey Hepburn's accent just sounded silly; she'd go on to do the PBS 'Gardens of the World with Audrey Hepburn' and there her somewhat irritating accent works as she walks through English gardens with aristocrats or waxes effusively about 'what I like most is when flowers go back to nature!' as in naturalized daffodils, but in an actual fictional movie, she just sounds ridiculous.<br /><br />To think that 'Breakfast at Tiffany's' was such a profound sort of light poetic thing with Audrey Hepburn one of the most beautiful women in the world--she was surely one of the most beautiful screen presences in 'My Fair Lady', matching Garbo in several things and Delphine Seyrig in 'Last Year at Marienbad.' And then this! And her final brief role as the angel 'Hap' in the Spielberg film 'Always' was just more of the lady stuff--corny, witless and stifling.<br /><br />I went to her memorial service at the Fifth Avenue Presbyterian Church, a beautiful service which included a boys' choir singing the Shaker hymn 'Simple Gifts.' The only thing not listed in the program was the sudden playing of Hepburn's singing 'Moon River' on the fire escape in 'Breakfast at Tiffany's,' and this brought much emotion and some real tears out in the congregation.<br /><br />A great lady who was once a fine actress (as in 'The Nun's Story') and one of the greatest and most beautiful of film stars in many movies of the 50's and 60's who became a truly bad one--that's not all that common. And perhaps it is only a great human being who, in making such things as film performances trivial, nevertheless has the largeness of mind to want to have the flaws pointed out mercilessly--which all of her late film work contained in abundance. Most of the talk about Hepburn's miscasting is about 'My Fair Lady.' But the one that should have had the original actress in it was 'Wait Until Dark,' which had starred Lee Remick on Broadway. Never as celebrated as Hepburn, she was a better actress in many ways (Hepburn was completely incapable of playing anything really sordid), although Hepburn was at least adequate enough in that part. After that, all of her acting went downhill.\n",
            "레이블 (0:부정, 1:긍정): 0\n"
          ]
        }
      ],
      "source": [
        "sample = dataset[\"train\"][5]\n",
        "print(f\"리뷰 내용 : {sample['text']}\")\n",
        "print(f\"레이블 (0:부정, 1:긍정): {sample['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX용 BERT 모델 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 토크나이저 로드 완료!\n",
            "어휘 크기: 30522\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "print(\"✅ 토크나이저 로드 완료!\")\n",
        "print(f\"어휘 크기: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MLX BERT 모델 생성 완료!\n",
            "모델 파라미터 수: 0\n",
            "모델 구조: MLXDistilBERT(\n",
            "  (embeddings): Embedding(30522, 768)\n",
            "  (position_embeddings): Embedding(512, 768)\n",
            "  (token_type_embeddings): Embedding(2, 768)\n",
            "  (embeddings_layer_norm): LayerNorm(768, eps=1e-05, affine=True)\n",
            "  (embeddings_dropout): Dropout(p=0.09999999999999998)\n",
            "  (lstm): LSTM(input_dims=768, hidden_size=768, bias=True)\n",
            "  (pre_classifier): Linear(input_dims=768, output_dims=768, bias=True)\n",
            "  (classifier): Linear(input_dims=768, output_dims=2, bias=True)\n",
            "  (dropout): Dropout(p=0.09999999999999998)\n",
            ")\n",
            "임베딩 크기: (30522, 768)\n",
            "트랜스포머 레이어 수: 6\n",
            "분류 헤드: (2, 768)\n",
            "\\n🧪 모델 테스트:\n",
            "✅ 모델 정상 작동! 출력 형태: (1, 2)\n"
          ]
        }
      ],
      "source": [
        "class MLXDistilBERT(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_labels=2, num_layers=6, num_heads=12):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 임베딩 레이어\n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(512, hidden_size)  # 최대 512 토큰\n",
        "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
        "        \n",
        "        # 레이어 정규화\n",
        "        self.embeddings_layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.embeddings_dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        # 간단한 LSTM 기반 모델로 대체 (MLX 호환성)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers)\n",
        "        \n",
        "        # 분류 헤드\n",
        "        self.pre_classifier = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def __call__(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # 임베딩\n",
        "        seq_len = input_ids.shape[1]\n",
        "        position_ids = mx.arange(seq_len)[None, :]\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = mx.zeros_like(input_ids)\n",
        "            \n",
        "        embeddings = (\n",
        "            self.embeddings(input_ids) +\n",
        "            self.position_embeddings(position_ids) +\n",
        "            self.token_type_embeddings(token_type_ids)\n",
        "        )\n",
        "        \n",
        "        embeddings = self.embeddings_layer_norm(embeddings)\n",
        "        embeddings = self.embeddings_dropout(embeddings)\n",
        "        \n",
        "        # LSTM으로 시퀀스 처리\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        \n",
        "        # 분류 헤드\n",
        "        pooled_output = lstm_out[:, 0]  # 첫 번째 토큰 사용\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = nn.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# 모델 생성\n",
        "model = MLXDistilBERT()\n",
        "print(\"✅ MLX BERT 모델 생성 완료!\")\n",
        "\n",
        "# MLX 파라미터 수 계산 (올바른 방법)\n",
        "def count_mlx_parameters(model):\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        if hasattr(param, 'size'):\n",
        "            total += param.size\n",
        "        elif hasattr(param, 'shape'):\n",
        "            total += np.prod(param.shape)\n",
        "    return total\n",
        "\n",
        "total_params = count_mlx_parameters(model)\n",
        "print(f\"모델 파라미터 수: {total_params:,}\")\n",
        "\n",
        "# 모델 구조 확인\n",
        "print(f\"모델 구조: {model}\")\n",
        "print(f\"임베딩 크기: {model.embeddings.weight.shape}\")\n",
        "print(f\"트랜스포머 레이어 수: 6\")\n",
        "print(f\"분류 헤드: {model.classifier.weight.shape}\")\n",
        "\n",
        "# 간단한 테스트\n",
        "print(f\"\\\\n🧪 모델 테스트:\")\n",
        "test_input = mx.array([[101, 102, 103, 104, 105]])  # 간단한 토큰 ID\n",
        "test_mask = mx.array([[1, 1, 1, 1, 1]])\n",
        "try:\n",
        "    output = model(test_input, test_mask)\n",
        "    print(f\"✅ 모델 정상 작동! 출력 형태: {output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 모델 오류: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 데이터 전처리 (MLX용)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 데이터 토큰화 중...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 40/40 [00:00<00:00, 4449.84 examples/s]\n",
            "Map: 100%|██████████| 10/10 [00:00<00:00, 2333.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 토큰화 완료!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize_for_mlx(batch):\n",
        "    # 토큰화\n",
        "    encoded = tokenizer(\n",
        "        batch[\"text\"], \n",
        "        padding=True, \n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"],\n",
        "        \"attention_mask\": encoded[\"attention_mask\"],\n",
        "        \"labels\": np.array(batch[\"label\"])\n",
        "    }\n",
        "\n",
        "# 데이터셋 토큰화\n",
        "print(\"🔄 데이터 토큰화 중...\")\n",
        "dataset = dataset.map(tokenize_for_mlx, batched=True)\n",
        "print(\"✅ 토큰화 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 훈련 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 훈련 설정 완료!\n",
            "배치 크기: 8, 에포크: 15\n",
            "옵티마이저: AdamW (lr=2e-5)\n"
          ]
        }
      ],
      "source": [
        "def loss_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    loss = nn.losses.cross_entropy(logits, labels)\n",
        "    return mx.mean(loss)\n",
        "\n",
        "def eval_fn(model, inputs, labels):\n",
        "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    predictions = mx.argmax(logits, axis=1)\n",
        "    accuracy = mx.mean(predictions == labels)\n",
        "    return accuracy\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = optim.AdamW(learning_rate=2e-5, weight_decay=0.01)\n",
        "\n",
        "# 훈련 설정\n",
        "batch_size = 8\n",
        "num_epochs = 15\n",
        "print(f\"✅ 훈련 설정 완료!\")\n",
        "print(f\"배치 크기: {batch_size}, 에포크: {num_epochs}\")\n",
        "print(f\"옵티마이저: AdamW (lr=2e-5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 파인튜닝 실행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 MLX 파인튜닝 시작!\n",
            "Apple M4 Pro 최적화로 빠른 학습 예상...\n",
            "✅ 훈련 데이터 준비 완료!\n",
            "입력 형태: (40, 512)\n",
            "레이블 형태: (40,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MLX 훈련 진행률: 100%|██████████| 15/15 [00:48<00:00,  3.23s/epoch, Epoch=15, Loss=0.000122, Avg Loss=0.062502]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ MLX 훈련 완료!\n",
            "⏱️  총 소요시간: 48.53초\n",
            "📊 평균 손실: 0.062502\n",
            "🚀 초당 처리: 12.36 samples/sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"🚀 MLX 파인튜닝 시작!\")\n",
        "print(\"Apple M4 Pro 최적화로 빠른 학습 예상...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 훈련 데이터 준비 (numpy 배열로 먼저 변환)\n",
        "train_data = dataset[\"train\"]\n",
        "train_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(train_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(train_data[\"attention_mask\"]))\n",
        "}\n",
        "train_labels = mx.array(np.array(train_data[\"labels\"]))\n",
        "\n",
        "print(f\"✅ 훈련 데이터 준비 완료!\")\n",
        "print(f\"입력 형태: {train_inputs['input_ids'].shape}\")\n",
        "print(f\"레이블 형태: {train_labels.shape}\")\n",
        "\n",
        "# 훈련 루프\n",
        "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
        "total_steps = 0\n",
        "total_loss = 0\n",
        "\n",
        "with tqdm(total=num_epochs, desc=\"MLX 훈련 진행률\", unit=\"epoch\") as pbar:\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # 배치별 훈련\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            batch_inputs = {\n",
        "                \"input_ids\": train_inputs[\"input_ids\"][i:i+batch_size],\n",
        "                \"attention_mask\": train_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "            }\n",
        "            batch_labels = train_labels[i:i+batch_size]\n",
        "            \n",
        "            # 손실 계산 및 그래디언트\n",
        "            loss, grads = loss_and_grad_fn(model, batch_inputs, batch_labels)\n",
        "            \n",
        "            # 옵티마이저 업데이트\n",
        "            optimizer.update(model, grads)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            total_steps += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / (len(train_data) // batch_size)\n",
        "        total_loss += avg_loss\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Loss\": f\"{avg_loss:.6f}\",\n",
        "            \"Avg Loss\": f\"{total_loss / (epoch + 1):.6f}\"\n",
        "        })\n",
        "        pbar.update(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n✅ MLX 훈련 완료!\")\n",
        "print(f\"⏱️  총 소요시간: {end_time - start_time:.2f}초\")\n",
        "print(f\"📊 평균 손실: {total_loss / num_epochs:.6f}\")\n",
        "print(f\"🚀 초당 처리: {len(train_data) * num_epochs / (end_time - start_time):.2f} samples/sec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MLX 모델 평가 및 추론\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 테스트 데이터 준비 완료!\n",
            "입력 형태: (10, 512)\n",
            "레이블 형태: (10,)\n",
            "🎯 테스트 정확도: 1.0000 (100.00%)\n",
            "📊 예측 분포: [10  0]\n",
            "📊 실제 분포: [10  0]\n",
            "\n",
            "🔮 MLX 모델 감정 분석 결과:\n",
            "==================================================\n",
            "1. 문장: I can watch this all day.\n",
            "   예측: 부정 (확률: 0.9998)\n",
            "   부정: 0.9998, 긍정: 0.0002\n",
            "\n",
            "2. 문장: This movie is absolutely terrible.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "3. 문장: Amazing performance by the actors.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "4. 문장: I can watch this all day.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n",
            "5. 문장: I love this movie.\n",
            "   예측: 부정 (확률: 0.9999)\n",
            "   부정: 0.9999, 긍정: 0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터 평가 (numpy 배열로 먼저 변환)\n",
        "test_data = dataset[\"test\"]\n",
        "test_inputs = {\n",
        "    \"input_ids\": mx.array(np.array(test_data[\"input_ids\"])),\n",
        "    \"attention_mask\": mx.array(np.array(test_data[\"attention_mask\"]))\n",
        "}\n",
        "test_labels = mx.array(np.array(test_data[\"labels\"]))\n",
        "\n",
        "print(f\"✅ 테스트 데이터 준비 완료!\")\n",
        "print(f\"입력 형태: {test_inputs['input_ids'].shape}\")\n",
        "print(f\"레이블 형태: {test_labels.shape}\")\n",
        "\n",
        "# 예측 (MLX에서는 mx.eval()이 함수입니다)\n",
        "mx.eval(model)\n",
        "test_logits = model(test_inputs[\"input_ids\"], test_inputs[\"attention_mask\"])\n",
        "predictions = mx.argmax(test_logits, axis=1)\n",
        "accuracy = mx.mean(predictions == test_labels)\n",
        "\n",
        "print(f\"🎯 테스트 정확도: {accuracy.item():.4f} ({accuracy.item() * 100:.2f}%)\")\n",
        "print(f\"📊 예측 분포: {np.bincount(predictions.tolist(), minlength=2)}\")\n",
        "print(f\"📊 실제 분포: {np.bincount(test_labels.tolist(), minlength=2)}\")\n",
        "\n",
        "# 감정 분석 함수\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"텍스트 감정 분석\"\"\"\n",
        "    # 토큰화\n",
        "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
        "    \n",
        "    # MLX 배열로 변환\n",
        "    input_ids = mx.array(inputs[\"input_ids\"])\n",
        "    attention_mask = mx.array(inputs[\"attention_mask\"])\n",
        "    \n",
        "    # 예측 (MLX에서는 mx.eval()이 함수입니다)\n",
        "    mx.eval(model)\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    probabilities = mx.softmax(logits, axis=1)\n",
        "    prediction = mx.argmax(logits, axis=1)\n",
        "    \n",
        "    return {\n",
        "        \"prediction\": prediction.item(),\n",
        "        \"probabilities\": probabilities[0].tolist(),\n",
        "        \"sentiment\": \"긍정\" if prediction.item() == 1 else \"부정\"\n",
        "    }\n",
        "\n",
        "# 테스트 문장들\n",
        "test_sentences = [\n",
        "    \"I can watch this all day.\",\n",
        "    \"This movie is absolutely terrible.\",\n",
        "    \"Amazing performance by the actors.\",\n",
        "    \"I can watch this all day.\",\n",
        "    \"I love this movie.\"\n",
        "]\n",
        "\n",
        "print(\"\\n🔮 MLX 모델 감정 분석 결과:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    result = predict_sentiment(sentence, model, tokenizer)\n",
        "    print(f\"{i}. 문장: {sentence}\")\n",
        "    print(f\"   예측: {result['sentiment']} (확률: {result['probabilities'][result['prediction']]:.4f})\")\n",
        "    print(f\"   부정: {result['probabilities'][0]:.4f}, 긍정: {result['probabilities'][1]:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
