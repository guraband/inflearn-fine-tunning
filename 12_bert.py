#!/usr/bin/env python3
"""
BERT Fine-tuning for IMDB Sentiment Analysis
Converted from 12_bert.ipynb

This script demonstrates how to fine-tune a DistilBERT model for sentiment analysis
using the IMDB dataset with a small sample of 50 examples.
"""

import os
import time
from typing import Dict, Any, Optional

from datasets import load_dataset, DatasetDict
from sklearn.metrics import accuracy_score
from tqdm.auto import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)

# MPS ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ import
from mps_utils import warm_up_mps, check_mps_availability, get_optimal_device, print_device_info


def check_label_distribution(dataset, dataset_name="ìƒ˜í”Œ"):
    """ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“Š {dataset_name} ë ˆì´ë¸” ë¶„í¬ í™•ì¸:")
    print("=" * 50)

    # í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬
    train_labels = [dataset["train"][i]["label"]
                    for i in range(len(dataset["train"]))]
    train_positive = sum(1 for label in train_labels if label == 1)
    train_negative = sum(1 for label in train_labels if label == 0)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬
    test_labels = [dataset["test"][i]["label"]
                   for i in range(len(dataset["test"]))]
    test_positive = sum(1 for label in test_labels if label == 1)
    test_negative = sum(1 for label in test_labels if label == 0)

    print(f"í›ˆë ¨ ë°ì´í„°: ê¸ì • {train_positive}ê°œ, ë¶€ì • {train_negative}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: ê¸ì • {test_positive}ê°œ, ë¶€ì • {test_negative}ê°œ")
    print(f"ì „ì²´: ê¸ì • {train_positive +
          test_positive}ê°œ, ë¶€ì • {train_negative + test_negative}ê°œ")

    # ë ˆì´ë¸” 1ì¸ ë°ì´í„° ì°¾ê¸°
    print(f"\nğŸ” {dataset_name}ì—ì„œ ë ˆì´ë¸” 1(ê¸ì •) ë°ì´í„° í™•ì¸:")
    positive_samples = []
    for i in range(len(dataset["train"])):
        if dataset["train"][i]["label"] == 1:
            positive_samples.append(i)
            if len(positive_samples) >= 5:  # ì²˜ìŒ 5ê°œë§Œ ìˆ˜ì§‘
                break

    if positive_samples:
        print(f"   {dataset_name}ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ {len(positive_samples)}ê°œ ë°œê²¬!")
        for idx, sample_idx in enumerate(positive_samples):
            sample = dataset["train"][sample_idx]
            print(f"   [{idx+1}] ìƒ˜í”Œ {sample_idx}: {sample['text'][:80]}...")
    else:
        print(f"   âš ï¸  {dataset_name}ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    return {
        'train_positive': train_positive,
        'train_negative': train_negative,
        'test_positive': test_positive,
        'test_negative': test_negative,
        'positive_samples': positive_samples
    }


def check_full_dataset_label_distribution(full_dataset, total_train, total_test, total_size):
    """ì „ì²´ ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("\nğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬:")
    print("=" * 60)

    # ì „ì²´ í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
    print("ì „ì²´ í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸ ì¤‘...")
    full_train_positive = 0
    full_train_negative = 0

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (1000ê°œì”©)
    batch_size = 1000
    for i in range(0, total_train, batch_size):
        end_idx = min(i + batch_size, total_train)
        batch_labels = [full_dataset["train"][j]["label"]
                        for j in range(i, end_idx)]
        full_train_positive += sum(1 for label in batch_labels if label == 1)
        full_train_negative += sum(1 for label in batch_labels if label == 0)

        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"   ì§„í–‰ë¥ : {i}/{total_train} ({i/total_train*100:.1f}%)")

    # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ (ë°°ì¹˜ ì²˜ë¦¬)
    print("ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸ ì¤‘...")
    full_test_positive = 0
    full_test_negative = 0

    for i in range(0, total_test, batch_size):
        end_idx = min(i + batch_size, total_test)
        batch_labels = [full_dataset["test"][j]["label"]
                        for j in range(i, end_idx)]
        full_test_positive += sum(1 for label in batch_labels if label == 1)
        full_test_negative += sum(1 for label in batch_labels if label == 0)

        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"   ì§„í–‰ë¥ : {i}/{total_test} ({i/total_test*100:.1f}%)")

    print(f"ì „ì²´ í›ˆë ¨ ë°ì´í„°: ê¸ì • {full_train_positive:,}ê°œ ({full_train_positive/total_train *
          100:.1f}%), ë¶€ì • {full_train_negative:,}ê°œ ({full_train_negative/total_train*100:.1f}%)")
    print(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: ê¸ì • {full_test_positive:,}ê°œ ({full_test_positive/total_test *
          100:.1f}%), ë¶€ì • {full_test_negative:,}ê°œ ({full_test_negative/total_test*100:.1f}%)")
    print(f"ì „ì²´ ë°ì´í„°ì…‹: ê¸ì • {full_train_positive + full_test_positive:,}ê°œ ({(full_train_positive + full_test_positive)/total_size *
          100:.1f}%), ë¶€ì • {full_train_negative + full_test_negative:,}ê°œ ({(full_train_negative + full_test_negative)/total_size*100:.1f}%)")

    # ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
    print(f"\nğŸ” ë ˆì´ë¸” 1(ê¸ì •)ì´ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ í™•ì¸:")

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ì°¾ê¸° (ì „ì²´ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    first_positive_train = None
    print("   í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ê²€ìƒ‰ ì¤‘...")
    for i in range(total_train):  # ì „ì²´ í›ˆë ¨ ë°ì´í„° ê²€ìƒ‰
        if full_dataset["train"][i]["label"] == 1:
            first_positive_train = i
            break
        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"     ì§„í–‰ë¥ : {i}/{total_train} ({i/total_train*100:.1f}%)")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ì°¾ê¸° (ì „ì²´ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    first_positive_test = None
    print("   í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ê²€ìƒ‰ ì¤‘...")
    for i in range(total_test):  # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²€ìƒ‰
        if full_dataset["test"][i]["label"] == 1:
            first_positive_test = i
            break
        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"     ì§„í–‰ë¥ : {i}/{total_test} ({i/total_test*100:.1f}%)")

    # ê²°ê³¼ ì¶œë ¥
    if first_positive_train is not None:
        sample = full_dataset["train"][first_positive_train]
        print(f"   âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥: {first_positive_train}ë²ˆì§¸")
        print(f"      ë‚´ìš©: {sample['text'][:100]}...")
    else:
        print("   âŒ í›ˆë ¨ ë°ì´í„° ì „ì²´ì—ì„œ ë ˆì´ë¸” 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    if first_positive_test is not None:
        sample = full_dataset["test"][first_positive_test]
        print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥: {first_positive_test}ë²ˆì§¸")
        print(f"      ë‚´ìš©: {sample['text'][:100]}...")
    else:
        print("   âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²´ì—ì„œ ë ˆì´ë¸” 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    # ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    print(f"\nğŸ” ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1(ê¸ì •) ìƒ˜í”Œ í™•ì¸:")
    full_positive_samples = []

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (ì²˜ìŒ 5000ê°œ)
    print("   í›ˆë ¨ ë°ì´í„°ì—ì„œ ê¸ì • ìƒ˜í”Œ ê²€ìƒ‰ ì¤‘...")
    for i in range(min(5000, total_train)):
        if full_dataset["train"][i]["label"] == 1:
            full_positive_samples.append(("train", i))
            if len(full_positive_samples) >= 3:
                break

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ê²€ìƒ‰ (ì²˜ìŒ 5000ê°œ)
    if len(full_positive_samples) < 3:
        print("   í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê¸ì • ìƒ˜í”Œ ê²€ìƒ‰ ì¤‘...")
        for i in range(min(5000, total_test)):
            if full_dataset["test"][i]["label"] == 1:
                full_positive_samples.append(("test", i))
                if len(full_positive_samples) >= 5:
                    break

    if full_positive_samples:
        print(f"   ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ {len(full_positive_samples)}ê°œ ë°œê²¬!")
        for idx, (split, sample_idx) in enumerate(full_positive_samples):
            sample = full_dataset[split][sample_idx]
            print(
                f"   [{idx+1}] {split} ë°ì´í„° ìƒ˜í”Œ {sample_idx}: {sample['text'][:80]}...")
    else:
        print("   âš ï¸  ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("   ì´ëŠ” ë°ì´í„°ì…‹ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    return {
        'train_positive': full_train_positive,
        'train_negative': full_train_negative,
        'test_positive': full_test_positive,
        'test_negative': full_test_negative,
        'positive_samples': full_positive_samples,
        'first_positive_train': first_positive_train,
        'first_positive_test': first_positive_test
    }


def get_sample_size(total_size):
    """ì‚¬ìš©ìë¡œë¶€í„° ìƒ˜í”Œ í¬ê¸°ë¥¼ ì…ë ¥ë°›ìŠµë‹ˆë‹¤."""
    print(f"\nğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹: {total_size:,}ê°œ")
    print("=" * 50)
    print("ğŸ¯ í•™ìŠµì— ì‚¬ìš©í•  ìƒ˜í”Œ í¬ê¸°ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("   - ì—”í„°: ê¸°ë³¸ê°’ 1000ê°œ ì‚¬ìš©")
    print("   - 0: ì „ì²´ ë°ì´í„° ì‚¬ìš©")
    print("   - ìˆ«ì: í•´ë‹¹ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œ ì‚¬ìš©")
    print("=" * 50)

    while True:
        try:
            user_input = input("ìƒ˜í”Œ í¬ê¸° ì…ë ¥ (ì—”í„°=1000, 0=ì „ì²´): ").strip()

            if user_input == "":
                return 1000  # ê¸°ë³¸ê°’
            elif user_input == "0":
                return total_size  # ì „ì²´ ë°ì´í„°
            else:
                sample_size = int(user_input)
                if sample_size < 0:
                    print("âŒ ìŒìˆ˜ëŠ” ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
                elif sample_size > total_size:
                    print(
                        f"âŒ ì „ì²´ ë°ì´í„°({total_size:,}ê°œ)ë³´ë‹¤ í° ìˆ˜ëŠ” ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
                return sample_size
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì—”í„°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            continue


def get_checkpoint_info():
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ë°›ìŠµë‹ˆë‹¤."""
    print("\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì„¤ì •:")
    print("=" * 40)
    print("ğŸ¯ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("   - ì—”í„°: ìë™ ì €ì¥ (ì—í¬í¬ë§ˆë‹¤)")
    print("   - 0: ì €ì¥ ì•ˆí•¨")
    print("   - ìˆ«ì: N ì—í¬í¬ë§ˆë‹¤ ì €ì¥")
    print("=" * 40)

    while True:
        try:
            user_input = input(
                "ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸° (ì—”í„°=ìë™, 0=ì €ì¥ì•ˆí•¨, ìˆ«ì=Nì—í¬í¬ë§ˆë‹¤): ").strip()

            if user_input == "":
                return "auto"  # ìë™ ì €ì¥
            elif user_input == "0":
                return "no"  # ì €ì¥ ì•ˆí•¨
            else:
                save_every = int(user_input)
                if save_every < 1:
                    print("âŒ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
                return save_every
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì—”í„°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            continue


def find_latest_checkpoint(output_dir: str) -> Optional[str]:
    """ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    if not os.path.exists(output_dir):
        return None

    checkpoints = []
    for item in os.listdir(output_dir):
        if item.startswith("checkpoint-"):
            checkpoints.append(item)

    if not checkpoints:
        return None

    # ì²´í¬í¬ì¸íŠ¸ ë²ˆí˜¸ë¡œ ì •ë ¬
    checkpoints.sort(key=lambda x: int(x.split("-")[1]))
    latest_checkpoint = os.path.join(output_dir, checkpoints[-1])

    print(f"ğŸ” ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {len(checkpoints)}ê°œ")
    print(f"   ìµœì‹  ì²´í¬í¬ì¸íŠ¸: {latest_checkpoint}")

    return latest_checkpoint


def ask_resume_training() -> bool:
    """ì´ì–´ì„œ í•™ìŠµí• ì§€ ì‚¬ìš©ìì—ê²Œ ë¬»ìŠµë‹ˆë‹¤."""
    print("\nğŸ”„ ì´ì–´ì„œ í•™ìŠµí•˜ê¸°:")
    print("=" * 30)
    print("ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   - ì—”í„°: ì´ì–´ì„œ í•™ìŠµ")
    print("   - n: ìƒˆë¡œ ì‹œì‘")
    print("=" * 30)

    while True:
        user_input = input("ì´ì–´ì„œ í•™ìŠµí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì—”í„°=ì˜ˆ, n=ì•„ë‹ˆì˜¤): ").strip().lower()

        if user_input == "" or user_input == "y" or user_input == "yes":
            return True
        elif user_input == "n" or user_input == "no":
            return False
        else:
            print("âŒ ì—”í„° ë˜ëŠ” 'n'ì„ ì…ë ¥í•˜ì„¸ìš”.")
            continue


def load_imdb_dataset(full_dataset, total_size, sample_size=None) -> DatasetDict:
    """IMDB ë°ì´í„°ì…‹ì„ ì§€ì •ëœ í¬ê¸°ë¡œ ìƒ˜í”Œë§í•˜ì—¬ 8:2ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    print("ë°ì´í„°ì…‹ ìƒ˜í”Œë§ ì‹œì‘...")
    start_time = time.time()

    # ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ìƒ˜í”Œ í¬ê¸° ê²°ì •
    if sample_size is None:
        sample_size = get_sample_size(total_size)

    print(f"\nğŸ“Š ì„ íƒëœ ìƒ˜í”Œ í¬ê¸°: {sample_size:,}ê°œ")
    if sample_size == total_size:
        print("   - ì „ì²´ ë°ì´í„° ì‚¬ìš©")
    else:
        print(f"   - ì „ì²´ì˜ {sample_size/total_size*100:.2f}% ì‚¬ìš©")

    # tqdmì„ ì‚¬ìš©í•œ ë” ìì„¸í•œ í”„ë¡œê·¸ë ˆìŠ¤ë°”
    with tqdm(total=100, desc="ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬", unit="%",
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:

        if sample_size == total_size:
            # ì „ì²´ ë°ì´í„° ì‚¬ìš©
            dataset = full_dataset["train"].train_test_split(test_size=0.2)
        else:
            # ì§€ì •ëœ í¬ê¸°ë§Œí¼ ìƒ˜í”Œë§
            dataset = full_dataset["train"].select(
                range(sample_size)).train_test_split(test_size=0.2)

        # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì™„ë£Œ
        pbar.n = 100
        pbar.refresh()

    end_time = time.time()
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"ğŸ“ ìºì‹œ ìœ„ì¹˜: ./data_cache")
    print(f"ğŸ“Š ì‚¬ìš©ëœ ìƒ˜í”Œ: {len(dataset['train']) + len(dataset['test'])}ê°œ")
    print(f"ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ: {len(dataset['train'])}ê°œ, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {
          len(dataset['test'])}ê°œ")

    # ìƒ˜í”Œ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    sample_distribution = check_label_distribution(dataset, "ìƒ˜í”Œ")

    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 10ê°œ)
    print("\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 10ê°œ):")
    print("=" * 80)
    for i in range(min(10, len(dataset["train"]))):
        sample = dataset["train"][i]
        print(f"\n[{i+1}] ë¦¬ë·° ë‚´ìš©: {sample['text'][:100]}...")  # ì²˜ìŒ 100ìë§Œ í‘œì‹œ
        print(f"    ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): {sample['label']}")
        print(f"    í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample['text'])}ì")

    print(f"\në°ì´í„°ì…‹ ë¶„í• : {list(dataset.keys())}")  # ['train', 'test']

    return dataset


def load_full_dataset_info():
    """ì „ì²´ IMDB ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ” ì „ì²´ IMDB ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸ ì¤‘...")
    full_dataset = load_dataset("imdb", cache_dir="./data_cache")
    total_train = len(full_dataset["train"])
    total_test = len(full_dataset["test"])
    total_size = total_train + total_test

    print(f"ğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹: {total_size:,}ê°œ")
    print(f"   - í›ˆë ¨ ë°ì´í„°: {total_train:,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {total_test:,}ê°œ")

    return full_dataset, total_train, total_test, total_size


def load_model_and_tokenizer():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ===")

    # MPS warm-up ë¨¼ì € ìˆ˜í–‰
    warm_up_mps()

    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased")

    print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
    return tokenizer, model


def preprocess_data(dataset: DatasetDict, tokenizer) -> DatasetDict:
    """ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ===")

    def tokenize(batch):
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•©ë‹ˆë‹¤."""
        return tokenizer(batch["text"], padding=True, truncation=True)

    dataset = dataset.map(tokenize, batched=True)
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    return dataset


def setup_trainer(model, dataset: DatasetDict, checkpoint_config="auto", resume_from_checkpoint=None) -> Trainer:
    """í›ˆë ¨ ì„¤ì • ë° Trainerë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    print("\n=== í›ˆë ¨ ì„¤ì • êµ¬ì„± ===")

    def compute_metrics(eval_pred):
        """í‰ê°€ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        logits, labels = eval_pred
        predictions = logits.argmax(axis=-1)
        acc = accuracy_score(labels, predictions)
        return {"accuracy": acc}

    # ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ì„¤ì •
    train_size = len(dataset["train"])
    total_samples = train_size + len(dataset["test"])

    print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {total_samples:,}ê°œ (í›ˆë ¨: {train_size:,}ê°œ)")

    # ìƒ˜í”Œ í¬ê¸°ì— ë”°ë¥¸ ì„¤ì • ì¡°ì •
    if total_samples <= 1000:
        # ì†Œê·œëª¨ ë°ì´í„°ì…‹ (1000ê°œ ì´í•˜)
        batch_size = 16
        num_epochs = 15
        logging_steps = 5
        print("   - ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì„¤ì • ì ìš©")
    elif total_samples <= 10000:
        # ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ (1000-10000ê°œ)
        batch_size = 32
        num_epochs = 10
        logging_steps = 10
        print("   - ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ ì„¤ì • ì ìš©")
    else:
        # ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (10000ê°œ ì´ìƒ)
        batch_size = 64
        num_epochs = 5
        logging_steps = 50
        print("   - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì„¤ì • ì ìš©")

    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    output_dir = "checkpoints"
    save_strategy = "no"
    save_steps = None

    if checkpoint_config == "auto":
        save_strategy = "epoch"
        print("   - ì²´í¬í¬ì¸íŠ¸: ì—í¬í¬ë§ˆë‹¤ ìë™ ì €ì¥")
    elif checkpoint_config == "no":
        print("   - ì²´í¬í¬ì¸íŠ¸: ì €ì¥ ì•ˆí•¨")
    elif isinstance(checkpoint_config, int):
        save_strategy = "steps"
        save_steps = checkpoint_config * \
            (train_size // batch_size)  # ì—í¬í¬ë‹¹ ìŠ¤í… ìˆ˜ ê³„ì‚°
        print(f"   - ì²´í¬í¬ì¸íŠ¸: {checkpoint_config} ì—í¬í¬ë§ˆë‹¤ ì €ì¥ (ë§¤ {save_steps} ìŠ¤í…)")

    # M4 Proì— ìµœì í™”ëœ ì„¤ì •
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        report_to="none",  # ì™¸ë¶€ ë¡œê¹…íˆ´ ë¹„í™œì„±í™”
        logging_steps=logging_steps,
        dataloader_pin_memory=False,  # MPSì—ì„œëŠ” pin_memory ë¹„í™œì„±í™”
        dataloader_num_workers=0,     # MPSì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        save_strategy=save_strategy,
        save_steps=save_steps,
        eval_strategy="no",           # í‰ê°€ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        learning_rate=2e-5,           # í•™ìŠµë¥  ëª…ì‹œì  ì„¤ì •
        weight_decay=0.01,            # ê°€ì¤‘ì¹˜ ê°ì‡  ì¶”ê°€
        load_best_model_at_end=False,  # ìµœê³  ëª¨ë¸ ë¡œë“œ ë¹„í™œì„±í™”
        save_total_limit=3,           # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        compute_metrics=compute_metrics,  # ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
    )

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ
    if resume_from_checkpoint:
        print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ: {resume_from_checkpoint}")
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)
        return trainer

    print("âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!")
    print(f"ğŸš€ M4 Pro ìµœì í™” ì„¤ì • ì ìš©ë¨! (ë°°ì¹˜í¬ê¸°: {batch_size}, ì—í¬í¬: {num_epochs})")
    return trainer


def train_model(trainer: Trainer) -> Dict[str, Any]:
    """ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ í›ˆë ¨ ===")
    print("í›ˆë ¨ ì‹œì‘...")

    train_result = trainer.train()

    print("í›ˆë ¨ ì™„ë£Œ!")
    print(f"í›ˆë ¨ ê²°ê³¼: {train_result}")

    return train_result


def save_final_model(trainer: Trainer, model_name: str = "fine_tuned_bert"):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"\nğŸ’¾ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")

    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    model_dir = f"models/{model_name}"
    os.makedirs(model_dir, exist_ok=True)

    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
    trainer.save_model(model_dir)
    trainer.tokenizer.save_pretrained(model_dir)

    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
    print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {model_dir}/")
    print(f"   ğŸ“„ íŒŒì¼ë“¤:")
    print(f"      - config.json (ëª¨ë¸ ì„¤ì •)")
    print(f"      - model.safetensors (ëª¨ë¸ ê°€ì¤‘ì¹˜)")
    print(f"      - tokenizer.json (í† í¬ë‚˜ì´ì €)")
    print(f"      - tokenizer_config.json (í† í¬ë‚˜ì´ì € ì„¤ì •)")
    print(f"      - vocab.txt (ì–´íœ˜ ì‚¬ì „)")

    return model_dir


def load_saved_model(model_path: str):
    """ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ğŸ“‚ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None


def test_predictions(tokenizer, model):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")

    # ìµœì  ë””ë°”ì´ìŠ¤ í™•ì¸
    device = get_optimal_device()
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_texts = [
        "I would put this at the top of the list of films in the category of unwatchable trash.",
        "I can watch this all day.",
        "This movie is absolutely fantastic!",
        "Terrible acting and boring plot."
    ]

    for text in test_texts:
        print(f"\ní…ìŠ¤íŠ¸: {text}")

        # "pt": pytorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ì „ë‹¬
        inputs = tokenizer(text, return_tensors="pt").to(device)

        outputs = model(**inputs)
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
        predictions = outputs.logits.argmax(dim=-1)

        print(f"ì˜ˆì¸¡ ë ˆì´ë¸”: {predictions[0]}")
        print(f"ì˜ˆì¸¡ í™•ë¥ : {outputs.logits[0][predictions[0]].item()}")
        print("ê¸ì •" if predictions[0] == 1 else "ë¶€ì •")

        # ë¡œì§“ ê°’ ì¶œë ¥
        print(f"ë¡œì§“ ê°’: {outputs.logits[0]}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ BERT Fine-tuning for IMDB Sentiment Analysis ì‹œì‘!")
    print("=" * 60)

    # 0. ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
    print_device_info()

    # 1. ì „ì²´ ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ
    full_dataset, total_train, total_test, total_size = load_full_dataset_info()

    # 2. ì „ì²´ ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    # full_distribution = check_full_dataset_label_distribution(
    #     full_dataset, total_train, total_test, total_size)

    # 3. ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    dataset = load_imdb_dataset(full_dataset, total_size)

    # 4. ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    checkpoint_config = get_checkpoint_info()

    # 5. ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    output_dir = "checkpoints"
    latest_checkpoint = find_latest_checkpoint(output_dir)
    resume_from_checkpoint = None

    if latest_checkpoint:
        if ask_resume_training():
            resume_from_checkpoint = latest_checkpoint
        else:
            print("ğŸ”„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.")

    # 6. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer, model = load_model_and_tokenizer()

    # 7. ë°ì´í„° ì „ì²˜ë¦¬
    dataset = preprocess_data(dataset, tokenizer)

    # 8. í›ˆë ¨ ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ í¬í•¨)
    trainer = setup_trainer(
        model, dataset, checkpoint_config, resume_from_checkpoint)

    # 9. ëª¨ë¸ í›ˆë ¨ (ì´ë¯¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµí–ˆë‹¤ë©´ ê±´ë„ˆëœ€)
    if not resume_from_checkpoint:
        train_result = train_model(trainer)
    else:
        print("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 10. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
    model_name = f"bert_imdb_{
        len(dataset['train']) + len(dataset['test'])}samples"
    saved_model_dir = save_final_model(trainer, model_name)

    # 11. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    test_predictions(tokenizer, model)

    print("\n" + "=" * 60)
    print("ğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")
    if checkpoint_config != "no":
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {output_dir}/")
        print("   ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì´ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {saved_model_dir}/")
    print("   ì´ ëª¨ë¸ì„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
