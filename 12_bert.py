#!/usr/bin/env python3
"""
BERT Fine-tuning for IMDB Sentiment Analysis
Converted from 12_bert.ipynb

This script demonstrates how to fine-tune a DistilBERT model for sentiment analysis
using the IMDB dataset with a small sample of 50 examples.
"""

import time
from typing import Dict, Any

from datasets import load_dataset, DatasetDict
from sklearn.metrics import accuracy_score
from tqdm.auto import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)


def load_imdb_dataset() -> DatasetDict:
    """IMDB ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  8:2ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    print("ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘...")
    start_time = time.time()

    print("ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    print("   (ì²˜ìŒ ì‹¤í–‰ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print("   (ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ë¡œì»¬ ìºì‹œì—ì„œ ë¹ ë¥´ê²Œ ë¡œë”©ë©ë‹ˆë‹¤)")

    # tqdmì„ ì‚¬ìš©í•œ ë” ìì„¸í•œ í”„ë¡œê·¸ë ˆìŠ¤ë°”
    with tqdm(total=100, desc="ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ ", unit="%",
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:

        # ë°ì´í„°ì…‹ ë¡œë”© (50ê°œ ìƒ˜í”Œ) - ë¡œì»¬ ìºì‹œ ì‚¬ìš©
        dataset = load_dataset(
            "imdb",
            split="train[:50]",
            cache_dir="./data_cache"  # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •
        ).train_test_split(test_size=0.2)

        # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì™„ë£Œ
        pbar.n = 100
        pbar.refresh()

    end_time = time.time()
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"ğŸ“ ìºì‹œ ìœ„ì¹˜: ./data_cache")
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {len(dataset['train']) + len(dataset['test'])}ê°œ")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(dataset['train'])
                       }ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(dataset['test'])}ê°œ")

    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
    sample = dataset["train"][5]
    print(f"ë¦¬ë·° ë‚´ìš©: {sample['text']}")
    print(f"ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): {sample['label']}")
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")

    return dataset


def load_model_and_tokenizer():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ===")

    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased")

    print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
    return tokenizer, model


def preprocess_data(dataset: DatasetDict, tokenizer) -> DatasetDict:
    """ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ===")

    def tokenize(batch):
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•©ë‹ˆë‹¤."""
        return tokenizer(batch["text"], padding=True, truncation=True)

    dataset = dataset.map(tokenize, batched=True)
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    return dataset


def setup_trainer(model, dataset: DatasetDict) -> Trainer:
    """í›ˆë ¨ ì„¤ì • ë° Trainerë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    print("\n=== í›ˆë ¨ ì„¤ì • êµ¬ì„± ===")

    def compute_metrics(eval_pred):
        """í‰ê°€ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        logits, labels = eval_pred
        predictions = logits.argmax(axis=-1)
        acc = accuracy_score(labels, predictions)
        return {"accuracy": acc}

    args = TrainingArguments(
        output_dir="test",
        per_device_train_batch_size=8,
        num_train_epochs=15,
        report_to="none",  # ì™¸ë¶€ ë¡œê¹…íˆ´ ë¹„í™œì„±í™”
        logging_steps=1,
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        compute_metrics=compute_metrics,  # ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
    )

    print("âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!")
    return trainer


def train_model(trainer: Trainer) -> Dict[str, Any]:
    """ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ í›ˆë ¨ ===")
    print("í›ˆë ¨ ì‹œì‘...")

    train_result = trainer.train()

    print("í›ˆë ¨ ì™„ë£Œ!")
    print(f"í›ˆë ¨ ê²°ê³¼: {train_result}")

    return train_result


def test_predictions(tokenizer, model):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_texts = [
        "I would put this at the top of the list of films in the category of unwatchable trash.",
        "I can watch this all day.",
        "This movie is absolutely fantastic!",
        "Terrible acting and boring plot."
    ]

    for text in test_texts:
        print(f"\ní…ìŠ¤íŠ¸: {text}")

        # "pt": pytorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ mpsì— ì „ë‹¬
        inputs = tokenizer(text, return_tensors="pt").to("mps")

        outputs = model(**inputs)
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
        predictions = outputs.logits.argmax(dim=-1)

        print(f"ì˜ˆì¸¡ ë ˆì´ë¸”: {predictions[0]}")
        print(f"ì˜ˆì¸¡ í™•ë¥ : {outputs.logits[0][predictions[0]].item()}")
        print("ê¸ì •" if predictions[0] == 1 else "ë¶€ì •")

        # ë¡œì§“ ê°’ ì¶œë ¥
        print(f"ë¡œì§“ ê°’: {outputs.logits[0]}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ BERT Fine-tuning for IMDB Sentiment Analysis ì‹œì‘!")
    print("=" * 60)

    # 1. ë°ì´í„° ë¡œë“œ
    dataset = load_imdb_dataset()

    # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer, model = load_model_and_tokenizer()

    # 3. ë°ì´í„° ì „ì²˜ë¦¬
    dataset = preprocess_data(dataset, tokenizer)

    # 4. í›ˆë ¨ ì„¤ì •
    trainer = setup_trainer(model, dataset)

    # 5. ëª¨ë¸ í›ˆë ¨
    train_result = train_model(trainer)

    # 6. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    test_predictions(tokenizer, model)

    print("\n" + "=" * 60)
    print("ğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
