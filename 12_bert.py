#!/usr/bin/env python3
"""
BERT Fine-tuning for IMDB Sentiment Analysis
Converted from 12_bert.ipynb

This script demonstrates how to fine-tune a DistilBERT model for sentiment analysis
using the IMDB dataset with a small sample of 50 examples.
"""

import time
from typing import Dict, Any

from datasets import load_dataset, DatasetDict
from sklearn.metrics import accuracy_score
from tqdm.auto import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)

# MPS ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ import
from mps_utils import warm_up_mps, check_mps_availability, get_optimal_device, print_device_info


def check_label_distribution(dataset, dataset_name="ìƒ˜í”Œ"):
    """ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“Š {dataset_name} ë ˆì´ë¸” ë¶„í¬ í™•ì¸:")
    print("=" * 50)

    # í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬
    train_labels = [dataset["train"][i]["label"]
                    for i in range(len(dataset["train"]))]
    train_positive = sum(1 for label in train_labels if label == 1)
    train_negative = sum(1 for label in train_labels if label == 0)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬
    test_labels = [dataset["test"][i]["label"]
                   for i in range(len(dataset["test"]))]
    test_positive = sum(1 for label in test_labels if label == 1)
    test_negative = sum(1 for label in test_labels if label == 0)

    print(f"í›ˆë ¨ ë°ì´í„°: ê¸ì • {train_positive}ê°œ, ë¶€ì • {train_negative}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: ê¸ì • {test_positive}ê°œ, ë¶€ì • {test_negative}ê°œ")
    print(f"ì „ì²´: ê¸ì • {train_positive +
          test_positive}ê°œ, ë¶€ì • {train_negative + test_negative}ê°œ")

    # ë ˆì´ë¸” 1ì¸ ë°ì´í„° ì°¾ê¸°
    print(f"\nğŸ” {dataset_name}ì—ì„œ ë ˆì´ë¸” 1(ê¸ì •) ë°ì´í„° í™•ì¸:")
    positive_samples = []
    for i in range(len(dataset["train"])):
        if dataset["train"][i]["label"] == 1:
            positive_samples.append(i)
            if len(positive_samples) >= 5:  # ì²˜ìŒ 5ê°œë§Œ ìˆ˜ì§‘
                break

    if positive_samples:
        print(f"   {dataset_name}ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ {len(positive_samples)}ê°œ ë°œê²¬!")
        for idx, sample_idx in enumerate(positive_samples):
            sample = dataset["train"][sample_idx]
            print(f"   [{idx+1}] ìƒ˜í”Œ {sample_idx}: {sample['text'][:80]}...")
    else:
        print(f"   âš ï¸  {dataset_name}ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    return {
        'train_positive': train_positive,
        'train_negative': train_negative,
        'test_positive': test_positive,
        'test_negative': test_negative,
        'positive_samples': positive_samples
    }


def check_full_dataset_label_distribution(full_dataset, total_train, total_test, total_size):
    """ì „ì²´ ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("\nğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬:")
    print("=" * 60)

    # ì „ì²´ í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
    print("ì „ì²´ í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸ ì¤‘...")
    full_train_positive = 0
    full_train_negative = 0

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (1000ê°œì”©)
    batch_size = 1000
    for i in range(0, total_train, batch_size):
        end_idx = min(i + batch_size, total_train)
        batch_labels = [full_dataset["train"][j]["label"]
                        for j in range(i, end_idx)]
        full_train_positive += sum(1 for label in batch_labels if label == 1)
        full_train_negative += sum(1 for label in batch_labels if label == 0)

        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"   ì§„í–‰ë¥ : {i}/{total_train} ({i/total_train*100:.1f}%)")

    # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ (ë°°ì¹˜ ì²˜ë¦¬)
    print("ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸ ì¤‘...")
    full_test_positive = 0
    full_test_negative = 0

    for i in range(0, total_test, batch_size):
        end_idx = min(i + batch_size, total_test)
        batch_labels = [full_dataset["test"][j]["label"]
                        for j in range(i, end_idx)]
        full_test_positive += sum(1 for label in batch_labels if label == 1)
        full_test_negative += sum(1 for label in batch_labels if label == 0)

        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"   ì§„í–‰ë¥ : {i}/{total_test} ({i/total_test*100:.1f}%)")

    print(f"ì „ì²´ í›ˆë ¨ ë°ì´í„°: ê¸ì • {full_train_positive:,}ê°œ ({full_train_positive/total_train *
          100:.1f}%), ë¶€ì • {full_train_negative:,}ê°œ ({full_train_negative/total_train*100:.1f}%)")
    print(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: ê¸ì • {full_test_positive:,}ê°œ ({full_test_positive/total_test *
          100:.1f}%), ë¶€ì • {full_test_negative:,}ê°œ ({full_test_negative/total_test*100:.1f}%)")
    print(f"ì „ì²´ ë°ì´í„°ì…‹: ê¸ì • {full_train_positive + full_test_positive:,}ê°œ ({(full_train_positive + full_test_positive)/total_size *
          100:.1f}%), ë¶€ì • {full_train_negative + full_test_negative:,}ê°œ ({(full_train_negative + full_test_negative)/total_size*100:.1f}%)")

    # ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
    print(f"\nğŸ” ë ˆì´ë¸” 1(ê¸ì •)ì´ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ í™•ì¸:")

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ì°¾ê¸° (ì „ì²´ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    first_positive_train = None
    print("   í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ê²€ìƒ‰ ì¤‘...")
    for i in range(total_train):  # ì „ì²´ í›ˆë ¨ ë°ì´í„° ê²€ìƒ‰
        if full_dataset["train"][i]["label"] == 1:
            first_positive_train = i
            break
        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"     ì§„í–‰ë¥ : {i}/{total_train} ({i/total_train*100:.1f}%)")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ì°¾ê¸° (ì „ì²´ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    first_positive_test = None
    print("   í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1 ê²€ìƒ‰ ì¤‘...")
    for i in range(total_test):  # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²€ìƒ‰
        if full_dataset["test"][i]["label"] == 1:
            first_positive_test = i
            break
        if i % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"     ì§„í–‰ë¥ : {i}/{total_test} ({i/total_test*100:.1f}%)")

    # ê²°ê³¼ ì¶œë ¥
    if first_positive_train is not None:
        sample = full_dataset["train"][first_positive_train]
        print(f"   âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥: {first_positive_train}ë²ˆì§¸")
        print(f"      ë‚´ìš©: {sample['text'][:100]}...")
    else:
        print("   âŒ í›ˆë ¨ ë°ì´í„° ì „ì²´ì—ì„œ ë ˆì´ë¸” 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    if first_positive_test is not None:
        sample = full_dataset["test"][first_positive_test]
        print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì´ ì²˜ìŒ ë“±ì¥: {first_positive_test}ë²ˆì§¸")
        print(f"      ë‚´ìš©: {sample['text'][:100]}...")
    else:
        print("   âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²´ì—ì„œ ë ˆì´ë¸” 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    # ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ í™•ì¸ (ë” ë„“ì€ ë²”ìœ„ì—ì„œ ê²€ìƒ‰)
    print(f"\nğŸ” ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1(ê¸ì •) ìƒ˜í”Œ í™•ì¸:")
    full_positive_samples = []

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (ì²˜ìŒ 5000ê°œ)
    print("   í›ˆë ¨ ë°ì´í„°ì—ì„œ ê¸ì • ìƒ˜í”Œ ê²€ìƒ‰ ì¤‘...")
    for i in range(min(5000, total_train)):
        if full_dataset["train"][i]["label"] == 1:
            full_positive_samples.append(("train", i))
            if len(full_positive_samples) >= 3:
                break

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ê²€ìƒ‰ (ì²˜ìŒ 5000ê°œ)
    if len(full_positive_samples) < 3:
        print("   í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê¸ì • ìƒ˜í”Œ ê²€ìƒ‰ ì¤‘...")
        for i in range(min(5000, total_test)):
            if full_dataset["test"][i]["label"] == 1:
                full_positive_samples.append(("test", i))
                if len(full_positive_samples) >= 5:
                    break

    if full_positive_samples:
        print(f"   ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œ {len(full_positive_samples)}ê°œ ë°œê²¬!")
        for idx, (split, sample_idx) in enumerate(full_positive_samples):
            sample = full_dataset[split][sample_idx]
            print(
                f"   [{idx+1}] {split} ë°ì´í„° ìƒ˜í”Œ {sample_idx}: {sample['text'][:80]}...")
    else:
        print("   âš ï¸  ì „ì²´ ë°ì´í„°ì—ì„œ ë ˆì´ë¸” 1ì¸ ìƒ˜í”Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("   ì´ëŠ” ë°ì´í„°ì…‹ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    return {
        'train_positive': full_train_positive,
        'train_negative': full_train_negative,
        'test_positive': full_test_positive,
        'test_negative': full_test_negative,
        'positive_samples': full_positive_samples,
        'first_positive_train': first_positive_train,
        'first_positive_test': first_positive_test
    }


def load_imdb_dataset() -> DatasetDict:
    """IMDB ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  8:2ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    print("ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘...")
    start_time = time.time()

    print("ğŸ“¥ IMDB ë°ì´í„°ì…‹ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    print("   (ì²˜ìŒ ì‹¤í–‰ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print("   (ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ë¡œì»¬ ìºì‹œì—ì„œ ë¹ ë¥´ê²Œ ë¡œë”©ë©ë‹ˆë‹¤)")

    # ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸
    print("ğŸ” ì „ì²´ IMDB ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸ ì¤‘...")
    full_dataset = load_dataset("imdb", cache_dir="./data_cache")
    total_train = len(full_dataset["train"])
    total_test = len(full_dataset["test"])
    total_size = total_train + total_test

    print(f"ğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹: {total_size:,}ê°œ")
    print(f"   - í›ˆë ¨ ë°ì´í„°: {total_train:,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {total_test:,}ê°œ")
    print(f"   - ìƒ˜í”Œ ì‚¬ìš©: 1000ê°œ (ì „ì²´ì˜ {1000/total_size*100:.2f}%)")

    # tqdmì„ ì‚¬ìš©í•œ ë” ìì„¸í•œ í”„ë¡œê·¸ë ˆìŠ¤ë°”
    with tqdm(total=100, desc="ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬", unit="%",
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:

        # 1000ê°œ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ ë¶„í• 
        dataset = full_dataset["train"].select(
            range(1000)).train_test_split(test_size=0.2)

        # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì™„ë£Œ
        pbar.n = 100
        pbar.refresh()

    end_time = time.time()
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"ğŸ“ ìºì‹œ ìœ„ì¹˜: ./data_cache")
    print(f"ğŸ“Š ì‚¬ìš©ëœ ìƒ˜í”Œ: {len(dataset['train']) + len(dataset['test'])}ê°œ")
    print(f"ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ: {len(dataset['train'])}ê°œ, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {
          len(dataset['test'])}ê°œ")

    # ìƒ˜í”Œ ë°ì´í„° ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    sample_distribution = check_label_distribution(dataset, "ìƒ˜í”Œ")

    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 10ê°œ)
    print("\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 10ê°œ):")
    print("=" * 80)
    for i in range(min(10, len(dataset["train"]))):
        sample = dataset["train"][i]
        print(f"\n[{i+1}] ë¦¬ë·° ë‚´ìš©: {sample['text'][:100]}...")  # ì²˜ìŒ 100ìë§Œ í‘œì‹œ
        print(f"    ë ˆì´ë¸” (0:ë¶€ì •, 1:ê¸ì •): {sample['label']}")
        print(f"    í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample['text'])}ì")

    print(f"\në°ì´í„°ì…‹ ë¶„í• : {list(dataset.keys())}")  # ['train', 'test']

    return dataset


def load_full_dataset_info():
    """ì „ì²´ IMDB ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ” ì „ì²´ IMDB ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸ ì¤‘...")
    full_dataset = load_dataset("imdb", cache_dir="./data_cache")
    total_train = len(full_dataset["train"])
    total_test = len(full_dataset["test"])
    total_size = total_train + total_test

    print(f"ğŸ“Š ì „ì²´ IMDB ë°ì´í„°ì…‹: {total_size:,}ê°œ")
    print(f"   - í›ˆë ¨ ë°ì´í„°: {total_train:,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {total_test:,}ê°œ")
    print(f"   - ìƒ˜í”Œ ì‚¬ìš©: 1000ê°œ (ì „ì²´ì˜ {1000/total_size*100:.2f}%)")

    return full_dataset, total_train, total_test, total_size


def load_model_and_tokenizer():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ===")

    # MPS warm-up ë¨¼ì € ìˆ˜í–‰
    warm_up_mps()

    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased")

    print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
    return tokenizer, model


def preprocess_data(dataset: DatasetDict, tokenizer) -> DatasetDict:
    """ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print("\n=== ë°ì´í„° ì „ì²˜ë¦¬ ===")

    def tokenize(batch):
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•©ë‹ˆë‹¤."""
        return tokenizer(batch["text"], padding=True, truncation=True)

    dataset = dataset.map(tokenize, batched=True)
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    return dataset


def setup_trainer(model, dataset: DatasetDict) -> Trainer:
    """í›ˆë ¨ ì„¤ì • ë° Trainerë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    print("\n=== í›ˆë ¨ ì„¤ì • êµ¬ì„± ===")

    def compute_metrics(eval_pred):
        """í‰ê°€ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        logits, labels = eval_pred
        predictions = logits.argmax(axis=-1)
        acc = accuracy_score(labels, predictions)
        return {"accuracy": acc}

    # M4 Proì— ìµœì í™”ëœ ì„¤ì • (1000ê°œ ìƒ˜í”Œìš©)
    args = TrainingArguments(
        output_dir="test",
        per_device_train_batch_size=32,  # 1000ê°œ ìƒ˜í”Œì— ë§ê²Œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        per_device_eval_batch_size=32,   # í‰ê°€ ë°°ì¹˜ í¬ê¸°ë„ ì¦ê°€
        num_train_epochs=10,             # 1000ê°œ ìƒ˜í”Œì´ë¯€ë¡œ ì—í¬í¬ ìˆ˜ ì¡°ì •
        report_to="none",  # ì™¸ë¶€ ë¡œê¹…íˆ´ ë¹„í™œì„±í™”
        logging_steps=10,  # ë¡œê¹… ë¹ˆë„ ì¡°ì •
        dataloader_pin_memory=False,  # MPSì—ì„œëŠ” pin_memory ë¹„í™œì„±í™”
        dataloader_num_workers=0,     # MPSì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        save_strategy="no",           # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        eval_strategy="no",           # í‰ê°€ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        learning_rate=2e-5,           # í•™ìŠµë¥  ëª…ì‹œì  ì„¤ì •
        weight_decay=0.01,            # ê°€ì¤‘ì¹˜ ê°ì‡  ì¶”ê°€
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        compute_metrics=compute_metrics,  # ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
    )

    print("âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ!")
    print("ğŸš€ M4 Pro ìµœì í™” ì„¤ì • ì ìš©ë¨!")
    return trainer


def train_model(trainer: Trainer) -> Dict[str, Any]:
    """ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤."""
    print("\n=== ëª¨ë¸ í›ˆë ¨ ===")
    print("í›ˆë ¨ ì‹œì‘...")

    train_result = trainer.train()

    print("í›ˆë ¨ ì™„ë£Œ!")
    print(f"í›ˆë ¨ ê²°ê³¼: {train_result}")

    return train_result


def test_predictions(tokenizer, model):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")

    # ìµœì  ë””ë°”ì´ìŠ¤ í™•ì¸
    device = get_optimal_device()
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_texts = [
        "I would put this at the top of the list of films in the category of unwatchable trash.",
        "I can watch this all day.",
        "This movie is absolutely fantastic!",
        "Terrible acting and boring plot."
    ]

    for text in test_texts:
        print(f"\ní…ìŠ¤íŠ¸: {text}")

        # "pt": pytorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # ì…ë ¥ëœ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ì „ë‹¬
        inputs = tokenizer(text, return_tensors="pt").to(device)

        outputs = model(**inputs)
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
        predictions = outputs.logits.argmax(dim=-1)

        print(f"ì˜ˆì¸¡ ë ˆì´ë¸”: {predictions[0]}")
        print(f"ì˜ˆì¸¡ í™•ë¥ : {outputs.logits[0][predictions[0]].item()}")
        print("ê¸ì •" if predictions[0] == 1 else "ë¶€ì •")

        # ë¡œì§“ ê°’ ì¶œë ¥
        print(f"ë¡œì§“ ê°’: {outputs.logits[0]}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ BERT Fine-tuning for IMDB Sentiment Analysis ì‹œì‘!")
    print("=" * 60)

    # 0. ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
    print_device_info()

    # 1. ì „ì²´ ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ
    full_dataset, total_train, total_test, total_size = load_full_dataset_info()

    # 2. ì „ì²´ ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    full_distribution = check_full_dataset_label_distribution(
        full_dataset, total_train, total_test, total_size)

    # 3. ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    dataset = load_imdb_dataset()

    # # 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    # tokenizer, model = load_model_and_tokenizer()

    # # 5. ë°ì´í„° ì „ì²˜ë¦¬
    # dataset = preprocess_data(dataset, tokenizer)

    # # 6. í›ˆë ¨ ì„¤ì •
    # trainer = setup_trainer(model, dataset)

    # # 7. ëª¨ë¸ í›ˆë ¨
    # train_result = train_model(trainer)

    # # 8. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    # test_predictions(tokenizer, model)

    print("\n" + "=" * 60)
    print("ğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
